%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Evaluation}
\seclabel{eval}
In this section, we compare the performance of ERMIA with SILO, a representative of the lightweight OCC camp, from two perspectives: 1) CC impact on performance and 2) scalability of underlying storage manager. 
The purposes of the performance evaluation are the followings. First, we want to show that the recent proposals for main-memory OLTP solutions with lightweight optimistic concurrency control schemes can perform well and are suitable for only on a limited subset of transactional applications, primarily due to limitations imposed by their (tightly-integrated) concurrency control mechanism. Second, the proposed ERMIA design offers a more efficient concurrency control, which allows its performance to remain high under contentious workloads. At the same time, ERMIA does not suffer significant sacrifices in performance, even in workloads where the specialized solutions shine. Lastly, we measure performance overhead of two CC schemes, SI and SSI, quantitatively. In addition to the CC impact on performance, we also show that ERMIA achieves multicore scalability thanks to scalable storage manager, mainly achieved by log manager, epoch resource manager and latch-free indirection array. 


\subsection{Experimental setup}
\seclabel{eval:setup}
% System
We used a 4-socket server with 6-core Intel Xeon E7-4807 processors, for a total of 24 physical cores and 48 hyperthreads. The machine has 64GB of RAM. We gave pre-faulted memory pool to both systems to avoid excessive page faults during runtime. The maximum number of worker threads are same with the number of physical cores in the machine, because it is enough to saturate CPUs in main-memory databases and the more workers contend for CPU, interferring each other. All worker threads were pinned to a dedicated core and NUMA node to minimize context switch penalty and inter-socket communication costs. Log records are written to tmpfs asynchronously and we used TCmalloc for memory allocation.

\subsection{TPC-E}
\seclabel{eval:tpce}
We first explore how ERMIA and SILO react to contention in the TPC-E which is a new standard OLTP benchmark. Although TPC-C has been dominantly used to evaluate OLTP systems performance for the past few decades, TPC-E was designed as a more realistic OLTP benchmark with modern features. TPC-E models brokerage firm activities and has more sophisticated schema model and transaction execution control. It is also known for higher read to write ratio than TPC-C. 

%TPCE + contention
Since TPC-E is not a heterogeneous workload, we introduce a new read-mostly analytic transaction that evaluates total assets of a customer accounts group and inserts the analytic activity log into \textit{analytic\_history} table. Total assets of the accounts are computed by joining \textit{holding\_summary} and \textit{last\_trade} tables. The vast majority of contentions occur between the analytic transaction and \textit{trade-result} transaction on \textit{customer\_account}, \textit{holding\_summary} and \textit{last\_trade} tables. 
The parameters we set for this experiment are 5000 customers, 500 scale factor and 10 initial trading days(the initial trading day parameter was limited by our machine's memory capacity). %We also replicate TPC-E's market emulator(MEE) and make it thread-local structure to avoid MEE's internal locking overhead. 
The analytic transaction takes 10\% out of the original TPC-E transaction mix. % TODO. new workload mix, footnote?

\labeledfigurewide{fig-tpce-robustness}{Total throughput (left); analytic transaction throughput(right) of TPCE-hybrid, varying contention degree}

% chart - Commit rates / Abort rates, varying contention degree. 
\figref{fig-tpce-robustness}(left) shows TPS with 24 worker threads under varying contention degree when we run TPC-E with the analytic transaction(TPCE-hybrid). To vary contention degree, we change the size of a customer account group to be scanned by the analytic transaction from 5\% to 20\% . The X-axis indicates the size of the target customer account group. SILO is slightly faster than ERMIA-SI at the small(5\%) contention level. As we increase contention degree, we can observe that ERMIA-SI outperforms SILO by ??\% and ??\%, at 10\% and 20\% contention level respectively. ERMIA-SSI delivers ??\% higher throughput than SILO, even with the expensive serializability guarantee cost. \kk{ this observation and numbers will be changed after experiments with more frequent analytic}
The main cause for the result is that the contention in the workload imposed heavy pressure on the SILO's OCC protocol; OCC enforced transactions to abort even if single tuple of their read-set is invalidated by updaters. Meanwhile, ERMIA endured the contention by protecting the read-sets from updaters effectively with snapshot isolation and distributed the contentions across multiple versions. we now focus on the throughput of the analytic transactions to see how well both systems support analytics on transactional data set.
%ERMIA-SI produces commits more than ten times than SILO at 20\% scan range. %TODO. put SSI number also.
As shown in the \figref{fig-tpce-robustness}(right), the analytic transactions were shoot down massively even under small contention degree on SILO. It is obviously negative implication to run heterogenous workload with OCC. ERMIA provides balanced query/transaction performance, while SILO extremely favors updaters. Thus, we can conclude that the OCC is not suitable to support emerging operational analytic workload. 

% scalability 
\labeledfigurewide{fig-tpce-scalability}{Throughput when running TPCE-hybrid(left); TPCE-original(right)}
\labeledfigurewide{fig-tpcc-scalability}{Throughput when running TPCC-contention(left); TPCC-original(right)}
In \figref{fig-tpce-scalability}(left), we fix contention degree to 10\% scan range and see scalability trends, increasing the number of workers. Overwhelmed by contentions, SILO does not achieve scalability, even with its excellent scalability of the underlying physical layer. ERMIA benefits from its robust CC scheme and its storage manager did not forestall achieving linear scalability. This figure shows that CC scheme's capability to deal with contention dictates overall performance in logical level, no matter how the underlying physical layer is scalable, in heterogeneous workload. We also performed the same experiment in the original TPC-E, without the analytic transaction. In \figref{fig-tpce-scalability}(right), we see lack of contentions did not take down SILO and both scalable storage managers scale reasonably over 24 cores. ERMIA-SI delivered similar throughput to SILO and ERMIA-SSI delivered ??\% lower peak performance. 

\subsection{TPC-C} 
\seclabel{eval:tpcc}
We also run TPC-C where lightweight OCC camp shine. This experiment will be focusing on evaluating storage manager's scalability, as TPC-C impose little pressure on CC scheme. TPC-C is well-known for update-heavy workload and small transaction footprints. Also, it is well-partitionable workload; conflicts can be reduced even further by partitioning and single-threadinging on the partition, setting ideal environment for the lightweight OCC. The only source of contention under partitioned TPC-C is multi-partition transaction, however, its impact on CC is dampened by small transaction footprints, avoiding the majority of read-write conflicts. % cross-partition transaction ratio, partition lock, 

% TPCC-original
We measure throughput of both systems in original TPC-C(TPCC-original) , varying the number of worker threads. All worker threads are given home warehouse and does not change the home warehouse during runtime. Cross partition transactinos take ??\%. \figref{fig-tpcc-scalability}(right) shows that both systems scale reasonably over 24 cores. Compared to SILO, ERMIA falls behind SILO by 20\% and ??\%, with SI and SSI respectively, at the peak performance for the following reasons. First, SILO has more lightweight physical layer as it goes by single-version store. Second, as we mentioned earlier, CC did not affect overall performance due to the lack of contention. Consequently, the lack of contention turned ERMIA's effort toward robustness into marginal overhead; the indirection array is at the center of this trade-off between TPC-C peak performance and robustness. The indirection array maintains snapshots to support CC scheme to distribute contentions. Whenver ERMIA accesses to database tuples, it has to chase pointers in the indirection array to find a visible version and incurs more cache footprints subsequently. ERMIA-SSI has extra cost to guarantee serializability; it keeps track of transaction dependency information and lookup them to identify presence of cyclic dependency. As we argued, the overhead is a cost to pursue our key design goal. We believe that robust performance for wider range of applications has greater value at the expense of insignificant performance loss in the specific workload.

% TPCC-random WH
We now change workers-warehouse binding. We enforced worker threads to pick a partition randomly, following non-uniform distribution, during runtime. The purpose of this modification is to bring reasonable amount of contentions in TPC-C and to see how the previous results change. We can see in the \figref{fig-tpcc-scalability}(right), the performance gap gets smaller than TPCC-original as SILO is affected by the contention again. In TPCC-contention, the throughput of SILO decreased by approximately 30\%, while ERMIA delivered 15\% slower performance than in TPCC-original. 

\subsection{Performance study}
\seclabel{eval:perf-study}

profiling and performance analysis with breakdown(pie or bar chart will look intuitively)

\begin{itemize}
\item cost of SI - indirection array  -10\%
\item cost of SSI - -15\%
\item physical layer's scalability : 1) Index - the biggest overhead 2) log mgr sustaints 24 core
\end{itemize}

While traversing the version chain, ERMIA has ?\% larger cache footprint and subsequent more cache misses than SILO. 

SSI overhead - serial checking overhead, phantom overhead. needs comparison with other SSI techniques.  

Scalability is achieved with various techniques collectively. Cache optimization, NUMA-aware memory management, epoch resource manager and latch-free indirection array help to achieve scalability. 

Log manager and total ordering overhead - marginal
Log manager was able to sustain 400K commits/s. we don't need to give up total ordering for scalability.
TID allocation and epoch manager's overhead was also negligible. 

