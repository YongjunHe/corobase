\documentclass[preprint]{sig-alternate-nocprt}
\usepackage[nocapsections,obey,smallbib]{fixacm}

%\usepackage{color}
\usepackage{graphicx}
%\usepackage{balance}
%\usepackage{subcaption}
%\usepackage{setspace}
\usepackage[hyphens]{url}

%\usepackage[bookmarks=false,hidelinks,pdftitle={Design Tradeoffs for Fast, Robust Database Systems}]{hyperref}

\pagestyle{empty}

\begin{document}
%\captionsetup[figure]{labelfont=bf,textfont=bf}
%\captionsetup[subfigure]{labelfont=normalfont,textfont=normalfont}

\title{Robust Concurrency Control for Heterogeneous Workloads on Modern Hardware}

\numberofauthors{2}
\author{
\alignauthor
%Tianzheng Wang\\
%       \affaddr{Department of Computer Science}\\
%       \affaddr{University of Toronto}\\
%       \email{{\large \sf tzwang@cs.toronto.edu}}
%\alignauthor
%Ryan Johnson\\
%       \affaddr{Department of Computer Science}\\
%       \affaddr{University of Toronto}\\
%       \email{{\large \sf ryan.johnson@cs.utoronto.ca}}
}

\maketitle

%1. new hw trend has led to new systems
The trend of large main memories and massively parallel processors has led to many new OLTP systems~\cite{HStore,Hyper,Hekaton,Silo} which focus on extracting the most performance benefits from new hardware. These systems utilize spacious main memory to fit the whole working set in DRAM with simplified, memory-friendly data structures, and takes advantage of multicore and multi-socket hardware to allow a much higher level of parallelism compared to conventional database systems.

%2. how current workloads make cc important again.
Meanwhile, database workloads are also evolving to become increasingly heterogeneous, blending the gap between transaction and analytical processing. Workloads nowadays are often mixed with reads and writes, showing a higher read to write ratio. Some of them become longer because of the analytical portion. The latest TPC-E~\cite{TPC-E} benchmark also reflected this trend, with a read to write ratio of 9.7:1, while the number for TPC-C is 1.9:1~\cite{TPC-Compare}. With modern parallel hardware, many more concurrent transactions are allowed to co-exist, possibly with overlapped footprints. The need to handle such heterogeneous workloads on modern hardware makes concurrency control (CC) an important area of research again.

%3. current schemes: 2PL blocks + deadlock issues. existing schemes in new systems (mostly OCC) suffer (1) long tx with other tx going on can't commit (2) write clobber read
Two representative CC mechanisms in existing systems are two-phase locking (2PL) and optimistic concurrency control (OCC). 2PL is usually found in most traditional disk-oriented systems, and OCC is adopted by more recent systems~\cite{Hekaton,Silo}. Though being correct and serializable, 2PL has always been criticized for having to block transactions and dealing with deadlocks. Recent systems have different architectures and designs to use OCC. But they all share a common weakness of having excessive false positives because \textit{any version overwritten before the reader commits will lead to an abort}. When handling heterogeneous workloads, long transactions tend to be aborted if other short, concurrent transactions overwrite any of its read set; short transactions with both reads and writes tend to cause each other to abort because of overlapped read/write sets. Ideally, the CC scheme should be \textit{robust} enough to avoid such weaknesses and produce reasonably well commit/abort ratios even in face of unbalanced heterogeneous workloads.

%4. attempts on new schemes such as SI/SSI, and 2PL+ wait depth limit
There have been some attempts to solve this problem with variants of snapshot isolation (SI) and 2PL. For example, serializable snapshot isolation~\cite{SSI} enhances the vanilla SI with seralizability guarantees and has been implemented in PostgreSQL. However, more efforts are still needed to make it scalable on modern parallel hardware~\cite{ScalableSSI}. 2PL with wait depth limited also shows promising results compared to strict 2PL. Another different approach is to maintain dependent transactions as a directed acyclic graph and only push delta results when transaction inputs change~\cite{LogicBlox}. Improvements on main stream CC schemes, such as multi-version concurrency control and basic time-stamp ordering and its variants such as partition-level locking also show promising results for certain workloads, it has been shown that none of the main stream CC schemes can scale well at a core count as high as one thousand~\cite{CCAbyss}.

%5. also arthictecture affects what CC a system can have. we need a system that enables good cc.
System architecture and design decisions also affect the design of CC schemes.

%6. a little on our system.


%In recent systems, approaches to logging and recovery include conventional (Hekaton~\cite{Hekaton}) and single-threaded (Hyper~\cite{Hyper}) centralized logging, request shipping and replication (HStore/VoltDB~\cite{HStore}), and epoch-based per-thread level distributed logging (Silo~\cite{Silo}). Traditionally, a centralized logging has been criticized as not able to scale gracefully on massively parallel hardware~\cite{AetherJournal} due to long critical sections, while it gives the nice feature of totally ordered log sequence numbers (LSNs) which simplifies both recovery and forward processing. To this end, we propose a disk-based centralized log that needs only a single compare-and-swap (CAS) to generate a totally ordered LSN. Transaction threads leave the critical section as long as they have acquired the log space. Using a single CAS, we not only avoid the long critical section problem, but also preserve the totally ordered LSN. Such an architecture with a totally ordered LSN enables us to do smarter concurrency control.

%With our new logging architecture, we adopt and improve serializable snapshot isolation~\cite{SSI} to tackle this problem. Transactions can be aborted as soon as conflicts are found, instead of waiting until the validation step. A transaction only acquires an LSN before pre-commit begins and does not write log records until a pre-commit succeeds. Only committed transactions' log records will be written in the log. This allows a decentralized validation step that multiple validations can proceed in parallel. In our system, all versions are accessed through a per-table indirection array that stores the object ID of each tuple's head version. Since log records are not written to the log until pre-commit succeeds, the log itself is the ``database'' in our system. Upon system restart, the recovery mechanism only needs to scan the log headers and refresh the indirection arrays from the latest checkpoint to get a fully working system.


\bibliographystyle{abbrv}
\bibliography{ref}
\end{document}
