%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{ERMIA: Fast and robust memory-optimized OLTP}
\seclabel{design}

In this section we describe the key pieces of ERMIA, with a focus on why we choose the design trade-offs we do.

%\subsection{Overview}
\tianzheng{blah blah...}
\subsection{Logging}
The log manager is a pivotal component in most database engines. It provides---if anything does---a centralized point of coordination that other pieces of the system build off of and depend on. ERMIA's log manager generates a commit log sequence number (LSN) for the transaction and reserves log buffer space for the transaction's log records with a \textit{single global atomic operation}. Achieving this required two key insights: first, the transaction can combine its log records into large blocks, avoiding the redundancy of writing individual log record headers and reducing the number of trips to the log. Second, the LSN space need not be contiguous as long as we can still convert easily between an LSN and the corresponding disk address.

The first property arises from our use of append-only storage. We achieve the second property by assigning each LSN to a ``segment'' and storing its segment number in the low order bits; the LSN's position on disk can be determined by looking up, and subtracting off, its segment's starting offset (read-only). Transactions race to ``open'' a new segment if they obtain an LSN past the end of the current one. Unlucky transactions holding an LSN in the gap between two segments can simply discard it and request a new one. Segments can be as large as 100GB or more, however, so overflows will be rare. Once an LSN and segment have been assigned, the transaction verifies availability of space in the log's circular memory buffer (again, read-only); only in case the buffer is full will transactions have to block pending space, but disk arrays can readily absorb the sequential write-only I/O stream.

An additional feature of the log is that transactions acquire a commit LSN before entering pre-commit, allowing validation of multiple transactions to proceed smoothly in parallel; depending on the outcome of pre-commit, a transaction either writes its log entries or a skip record (to abort) to the reserved log block.

Finally, because the shared counter is implemented as a wait-free linked list, the transaction can notify the log writer that its block is ready to be flushed by simply flagging its node as ``dead'' (a blind store). The log writer periodically scans the list and writes out all log blocks that precede the oldest ``live'' buffer allocation; transactions do not touch each others' nodes, and the log writer only reads them and flags dead nodes for the garbage collector.

\subsection{Epoch-based resource management}
\kk{mentioning problems without epoch mgr first?} We have developed a lightweight epoch management system that can track multiple timelines of differing granularities in parallel. A multi-transaction-scale epoch manager implements garbage collection of dead versions and deleted records, a medium-scale epoch manager implements read-copy-update (RCU) that manages physical memory and data structure usages~\cite{McKenneyS98}, and a very short timescale epoch manager tracks transaction IDs (TIDs), which we recycle aggressively (see details in \secref{tm}).

The key to efficiency here is to avoid flagging stragglers unless it is absolutely necessary (because coordinating with non-responsive threads is very expensive). Therefore, ERMIA does not attempt to reclaim resources for epoch $N$ until epoch $N+2$ begins. This way, potential stragglers have all of epoch $N+1$ to quiesce without penalty; however, epoch $N+3$ cannot begin until the last straggler from epoch $N$ completes. This four-phase scheme communicates far less with stragglers than the traditional two-phase \tianzheng{three-phase?} approach while maintaining the same worst-case timing bound. It allows us to track epochs at a very fine granularity when necessary. 

\kk{ How do we use the epoch for TID, RCU, and GC? algorithm description for TID alloc, RCU and GC epoch lifecycle }

\subsection{Transaction management}
\seclabel{tm}
Each transactions in the system is assigned a slot in a global transaction state table when it begins. This fixed-size table holds the transaction's begin time (which is the log's end LSN at the time it started), status, and end time (if applicable). TIDs are a combination of table offset and epoch, with an epoch manager to prevent entries from being recycled too soon. Update transactions write their TIDs into each version they create, change their status to pre-commit, acquire a commit LSN (or are given one by an impatient peer), and finally commit atomically by changing their status to ``committed.'' A post-commit cleanup step involves replacing the transaction's TID with its commit LSN, at which point the state table entry is no longer needed and can be recycled by the epoch manager. Other transactions that encounter a TID in a version can reliably verify its commit status and age by visiting the transaction state table, and---if necessary---will help a peer enter pre-commit by acquiring a log block on its behalf. \kk{figure will help in understanding this clearly}

\subsection{Indirection arrays}
\seclabel{design:oid}

The indirection arrays used in ERMIA are very similar to the ones proposed in the literature. All logical objects are identified by an object ID (OID) that maps to a slot in an OID array that contains the physical pointer to data. The pointer may reference disk, or a chain of versions stored in memory. As with Hekaton, uncommitted versions are never written to disk; but unlike Hekaton, we dispense with delta records (too expensive to apply) and use pure copy-on-write. New versions can be installed by an atomic compare-and-swap operation, and an uncommitted record at the head of the chain constitutes a write lock for CC schemes that care to track write-write conflicts (as most do). 

\subsection{Concurrency control}
\seclabel{design:cc}

ERMIA has been designed from the ground up to allow efficient implementations of a variety of CC mechanisms, including Silo/Hekaton flavored read-set validation and snapshot isolation. Moreover, it allows efficient implementations of non-trivial CC schemes (other than simplistic read-set validation) to handle heterogeneous workloads gracefully and make them serializable (e.g., complex financial transactions represented by TPC-E). The components in ERMIA work together to make this possible: indirection arrays allow cheap (almost free) multi-versioning; at an extremely low overhead, the log gives total commit ordering, which is the key to implement snapshot isolation; the transaction manager can help determine a version's age easily.

Depending on the targeted workload, ERMIA can use read set validation, two-phase commit, snapshot isolation, or even serializable snapshot isolation (SSI)~\cite{Cahill08RF}. We focus on handling \textit{read-mostly} workloads gracefully while maintaining comparable performance for short, update-intensive workloads. Although not serializable, snapshot isolation is an ideal choice to start from: long, read-mostly transactions will have much higher chance to survive when compared to read-set validation. ERMIA uses SSI to guarantee serializability. In particular, our variant of SSI takes advantage of ERMIA's design to avoid sacrificing much performance as virtually all existing SSI based systems do. Below we briefly introduce SSI and highlight our SSI variant for ERMIA.

SSI ensures serailizability by tracking the ``dangerous structure'' that must exist in every serial dependency cycle under SI as shown in Figure~\ref{ssi-dang-struct}~\cite{Cahill08RF}. In the figure, T1 or T2 must abort if the read-write dependency exists and T3 committed first. The original implementation uses 

\labeledfigure{ssi-dang-struct}{The ``dangerous structure'' that must exist in every serial dependency cycle under SI. A non-serializable schedule will have T1 and T2 read versions that are later overwritten by T2 and T3, respectively, with a T3 which overwrites a version that was read by T2 commits first.}

\textbf{Stamp-based tracking and parallel pre-commit.}
The original design of SSI uses a non-blocking read lock in each version to track reads, in addition to write-tracking. It requires sophisticated machinery to release the lock at the right time and even more efforts to improve its performance on multicores~\cite{HanPJFRY2014}.

{\bf Non-blocking read tracking.}
To track reads of a certain version when the overwriter's pre-commits, one might maintain a list of the TIDs of readers in each version. Readers will register themselves to the list when they read the version and deregister when they commit or abort. However, embedding a list in version headers will increase the size of a version significantly, causing more cache misses, especially for versions that are frequently read by multiple transactions (which is common in main-memory systems with massively parallel hardware). Since ERMIA executes each transaction from beginning to end by a single thread, without changing transaction context, we use a centralized TID list to record all in-flight transactions, and a bitmap in each old version to indicate the positions of readers in the centralized list. A transaction will register itself by setting the bitmap and putting its TID in the list's slot that corresponds to its thread ID. Note that the TID list is indexed by thread ID, so there is no contention upon transaction registration. The writer will be able to find out all readers through the bitmap and TID list at pre-commit.

\textbf{Read optimization.}
Tracking reads is a major culprit to the performance of SSI and most CC schemes that guarantee serializability. Existing optimizations usually focus on avoiding certain read-only transactions to participate in SSI~\cite{PortsG12}, which will not help reduce the overhead for \textit{read-mostly} transactions. In particular, most tuples in a database should be ``old'', i.e., not frequently updated. Long, read-mostly transactions will therefore have a higher chance of accessing these old tuples. Based on this observation, we optimize reads by guaranteeing any transaction that has read ``old'' versions can commit, unless it violates other conditions that necessitates its abort (e.g., a write-write conflict). In detail, a version's age is determined by subtracting its creation timestamp from the accessing transaction's begin timestamp. A version is old if its age is larger than a predefined threshold. Old versions will not be inserted to the read set. As a result, there is no way for the reader itself to go through SSI checks as usual during pre-commit. At pre-commit, if a transaction (the ``writer'') found itself overwrote an old version that was read by another transaction, it will have to abort, so that the readers can safely commit, avoiding forming the dangerous structure.

%To tell whether an old version has been read during the writer's pre-commit time, one might maintain a list of the TIDs of readers in each old version for the writer to check during pre-commit. However, embedding a list in version headers will increase the size of a version significantly, causing more cache misses, especially for versions that are frequently read by multiple transactions (which is common in main-memory systems with massively parallel hardware). Since ERMIA executes each transaction from beginning to end by a single thread, without changing transaction context, we use a centralized TID list to record all in-flight transactions, and a bitmap in each old version to indicate the positions of readers in the centralized TID list. Transaction threads will register themselves in the TID list upon start. Note that the TID list is indexed by thread ID, so there is no contention on the TID list upon thread registration. When a reader accesses an old version, it sets the corresponding bit in version's bitmap to indicate its existence. Then the writer will be able to find out all readers through the bitmap and TID list at pre-commit.

The above methodology could generate false positives: the TID slot indicated by the set bit in an old version's bitmap might already be re-allocated to another transaction. For example, the reader could have already committed by the time the writer enters pre-commit. Such a schedule does not indicate a cycle in the dependency graph (so far), but the writer still has to abort. To reduce such false positives, we employ a \texttt{bstamp} field in each old version to record the latest reader's begin time stamp. The writer will only abort if the reader found in the centralized TID list has a begin timestamp older than the version's \texttt{bstamp}, eliminating most false positives caused by TID slot reuse.

%\tianzheng{talk about phantom prot here or somewhere that talks about implementation? it's implementation specific--b/c we based on silo and masstree, we use this weird sort of phantom protection.}

{\bf Phantom protection.} ERMIA is amenable to various phantom protection mechanisms, such as hierarchical and key-range locking~\cite{KimuraGK12,Lomet93}. Since ERMIA's our current implementation is based on Silo~\cite{TuZKLM13}, we opt for the same tree-version validation mechanism---a lightweight, conservative approach---for fair comparison in the evaluation. The basic idea is to track and verify the index's (Masstree) node versions, taking advantage of the fact that structural modifications in Masstree will result in a version number change in affected leaf nodes. In addition to maintaining the read set, same as Silo, ERMIA also maintains a \textit{node set}, which maps from leaf nodes that fall into the range query to node versions. The node set is examined after read/write set validation. If any node's version does not match with the latest, the transaction will abort. Under SI, the verification can be done right before post-commit, while for SSI we verify the node set as the last step of pre-commit. Interested readers may refer to~\cite{TuZKLM13} for more details.

\subsection{Recovery}
\seclabel{design:recovery}

Recovery in ERMIA is straightforward because the log contains only committed work. OID arrays are the only real source of complexity, as they are volatile in-memory data structures that make it possible to find all other objects in the system. Logical objects (records) are physically logged, while physical data (allocator state and OID array contents) use logical logging. OID arrays are themselves objects stored in a master OID array, but they are updated in place to avoid overloading the log, with changes replayed by a log analysis step that reads only log block headers. This analysis step is very fast, because the skipped-over log payloads account for 90\% or more of the total log. In order to support efficient recovery, system transactions occasionally checkpoint the OID arrays using a fuzzy checkpointing mechanism to minimize the impact on user transactions. Because the log is the database, recovery only needs to rebuild the OID arrays in memory; anti-caching will take care of loading the actual data, though background pre-loading is highly recommended to minimize cold start effects.

%\subsection{Prototype Implementation}
%\seclabel{design:prototype}
%
%We implement a prototype of the ERMIA architecture and measure its performance.  For the implementation of the prototype we use a large fraction of the publicly available Silo codebase~\footnote{Silo's codebase can be downloaded from: https://github.com/stephentu/silo.}.  Silo uses the Masstree \cite{MaoKM12} as a cache-efficient index structure. 
