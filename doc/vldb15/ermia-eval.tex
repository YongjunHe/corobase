%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Evaluation}
\seclabel{eval}
In this section, we compare the performance of ERMIA and Silo, a representative of the lightweight OCC camp, from two perspectives: (1) the impacts of concurrency control scheme on performance and (2) scalability of the underlying storage manager. The purpose of our evaluation is two-fold. First, we show that although recent OCC proposals for main-memory OLTP perform extremely well, they are only suitable for a small fraction of transaction workloads---short update intensive transactions---primarily due to limitations imposed by their (tightly-integrated) concurrency control mechanism. Second, we demonstrate that ERMIA is able to offer robust concurrency control, maintaining high performance even under contentious workloads without sacrificing features. At the same time, ERMIA is also able to match the performance of specialized OCC solutions when running their favorite workloads. We also show that because of carefully orchestrated communications and interactions in both the physical and logical levels, ERMIA scales well under various types of workloads.

\subsection{Experimental setup}
\seclabel{eval:setup}
% System
We used a 4-socket server with 6-core Intel Xeon E7-4807 processors, for a total of 24 physical cores and 48 hyperthreads. The machine has 64GB of RAM. All worker threads were pinned to a dedicated core and NUMA node to minimize context switch penalty and inter-socket communication costs. Log records are written to tmpfs asynchronously. We measure the performance three systems: Silo, ERMIA with SI (ERMIA-SI), and ERMIA with SSI (ERMIA-SSI). Next we introduce the benchmarks.

\subsection{Benchmarks}
\seclabel{eval:benchmarks}
We run a microbenchmark, TPC-C and TPC-E benchmarks on all variants. In each run, we load data from scratch and run the benchmark for 40 seconds.

\textbf{Microbenchmark.}
We use the Stock table of TPC-C database (32 warehouses) for the microbenchmark transactions. The Stock table has 100K*Warehouses (3.2M) records. Each transaction randomly picks a subset of records to read and a smaller fraction of those to update.

\textbf{TPCE-original.}
TPC-E is a new standard OLTP benchmark. Although TPC-C has been dominantly used to evaluate OLTP systems performance for the past few decades, TPC-E was designed as a more realistic OLTP benchmark with modern features. TPC-E models brokerage firm activities and has more sophisticated schema model and transaction execution control\cite{TPCE}. It is also known for higher read to write ratio than TPC-C.


%TPCE + contention
\textbf{TPCE-hybrid.}
Since TPC-E is not a heterogeneous workload, we introduce a new read-mostly analytic transaction that evaluates aggregate assets of a random customer account group and inserts the analytic activity log into \textit{analytic\_history} table. Total assets of an account are computed by joining \textit{holding\_summary} and \textit{last\_trade} tables. The vast majority of contentions will occur between the analytic transaction and \textit{trade-result} transaction. The parameters we set for TPC-E experiments are 5000 customers, 500 scale factor and 10 initial trading days (the initial trading day parameter was limited by our machine's memory capacity). The followings are our new workload mix with the analytic transaction: broker-volume (4.9\%), customer-position (8\%), market-feed (1\%), market-watch (13\%), security-detail (14\%), trade-lookup (8\%), trade-order (10.1\%), trade-result (10\%), trade-status (9\%), trade-update (2\%) and analytic (20\%). 


\textbf{TPCC-fixed.}
TPC-C is well-known for update-heavy workload and small transaction footprints. Also, it is well-partitionable workload; conflicts can be reduced even further by partitioning and single-threadinging on local partitions, creating a nearly ideal environment for lightweight OCC schemes. In the partitioned TPC-C, the only source of contention is multi-partition transaction, however, its impact on CC is dampened by small transaction footprints, avoiding the majority of read-write conflicts. In TPCC-fixed, entire database is partitioned, all worker threads are given a local warehouse at benchmarking initialization, and does not change the partition binding during runtime. The fraction of cross partition transaction is 1\%. 


\textbf{TPCC-random.}
In TPCC-random, we enforce worker threads to pick a partition randomly during runtime, following non-uniform distribution. The purpose of this modification is to create a reasonable amount of contention in the workload.  

\subsection{TPC-E}
\seclabel{eval:tpce}

\labeledfigurewide{fig-tpce-robustness}{Normalized throughput (left); normalized analytic transaction throughput (right) of TPCE-hybrid, varying contention degree.}

% chart - Commit rates / Abort rates, varying contention degree. 
We first explore how ERMIA and Silo react to contention in heterogeneous workload. \figref{fig-tpce-robustness} (left) shows the normalized throughput with 24 worker threads, varying contention degree when we run TPC-E with the analytic transaction (TPCE-hybrid). To vary the degree of contention, we adjust the size of a customer account group to be scanned by the analytic transaction from 1\% to 60\%. At the lowest contention degree(1\%), ERMIA-SI and Silo deliver similar performance. However, SILO starts to lag behind ERMIA-SI from 5\% contention degree and the performance gap between them increases. Especially, at 60\% contention degree, SILO's throughput is only 59\% of ERMIA-SI throughput. If the analytic query takes more than 20\% in the workload mix, the performance gap will be even larger. ERMIA-SSI also does not collapse by contention. Even though it falls behind Silo by 26\% at 5\% contention degree, it starts to close the gap quickly and catches up with Silo at 20\% contention degree. However, ERMIA-SSI still suffers from serializability checking cost and larger cache footprint for transaction dependency tracking.
The main cause for the result is that the contention in the workload imposed heavy pressure on the OCC protocol; OCC enforced analytic queries to abort even if a small fraction of read-set is invalidated by updaters. Meanwhile, ERMIA endured the contention by protecting the queries from updaters effectively with snapshot isolation and distributed the contentions across multiple versions. 
we now take a closer look on the throughput of the analytic transactions to see how well both systems support heterogeneous workloads. \figref{fig-tpce-robustness} (right) illustrates normalized query throughput from the previous experiment. Even at the 1\% contention degree, Silo only commits 65\% as many queries as ERMIA-SI. As query size increases, Silo's queries become more vulnerable to updaters and query throughput drops sharply. These massive query aborts affected the overall throughput in the previous experiment, and would have an even larger impact if aborted queries were retried until successful rather than dropped. In recap, Silo's OCC scheme favors update transactions and transactions with large read footprints can starve in contentious workloads. Meanwhile ERMIA provides balanced query/transaction performance. This experiment shows that it is discouraging to serve emerging heterogeneous workload with OCC, even if overall throughput looks reasonable. 
   
% scalability 
\labeledfigurewide{fig-tpce-scalability}{Throughput when running TPCE-hybrid (left); TPCE-original (right).}
\labeledfigurewide{fig-tpcc-scalability}{Throughput when running TPCC-random (left); TPCC-fixed (right).}
In \figref{fig-tpce-scalability} (left), we fix contention degree to 20\% scan range and see scalability trends, increasing the number of workers. Overwhelmed by contention, Silo does not achieve linear scalability. ERMIA benefits from its robust CC scheme and its storage manager retains linear scalability. This figure shows that not only the scalability of underlying physical layer, but also CC scheme performance dictates overall performance. We also performed the same experiment in the original TPC-E, without the analytic transaction. As illustrated in \figref{fig-tpce-scalability} (right), both ERMIA-SI and Silo achieve linear scalability over 24 cores. Silo does not suffer from contention, since TPCE-original has insignificant contention. ERMIA-SSI delivers 83\% of ERMIA-SI performance with 24 threads due to the serializability cost.

\subsection{TPC-C} 
\seclabel{eval:tpcc}
We also run normal TPC-C, where lightweight OCC shines. This experiment focuses on evaluating the storage manager's natural scalability, as TPC-C imposes little pressure on the CC scheme. 

% TPCC-fixed
We measure throughput of all systems in TPCC-fixed, varying the number of worker threads. \figref{fig-tpcc-scalability} (right) shows that all systems scale reasonably over 24 cores. ERMIA falls behind Silo by 10\% and 20\%, with SI and SSI respectively, at the peak performance for the following reasons. First, OCC did not suppress throughput due to lack of contention. Second, Silo benefits from TPC-C's extremely small cache footprint (L2 resident). Unlike Silo, ERMIA maintains multi-versions in the indirection array, resulting in additional cache miss costs during indirection array traversals to find the desired version. This was invisible in the TPC-E because its large cache footprint already overwhelmed the processor caches. Thus, we can characterize the case where Silo outperforms ERMIA in workloads where 1) contention is rare and 2) transaction footprint is small enough to take advantage of processor caches; TPC-C is the compelling example of such workload. ERMIA-SSI had an additional 10\% cost to guarantee serializability.

% TPCC-random
We now run TPCC-random and see how the previous results change. As shown in the \figref{fig-tpcc-scalability} (right), we can see that Silo is more sensitive to contention than ERMIA. Compared to TPCC-fixed, the throughput of Silo decreased by approximately 30\%, while ERMIA lost only 15\% lower performance, catching up with Silo.
%
%\subsection{Performance study}
%\seclabel{eval:perf-study}
%We performed factor analysis to figure out how much cost ERMIA pays for its critical components quantitatively. \figref{fig-tpcc-cycle} illustrates normalized CPU cycle breakdown in the TPCC-partitioned with 24 worker threads. As shown in the figure, the indirection array imposed 10\% extra cost which mainly came from last level cache misses; 32\% of cache misses occured in indirection array. In ERMIA-SSI, serializability check brought additional 10\% overhead.\kk{not sure about this number} In all systems, Masstree is the biggest bottleneck with more than 30\% out of total cycles. The overhead of log manager and total ordering were negligible(~4\%). This supports that fully-uncoordinated logging and abandonment of total order are not necessarily required to achieve scalability. Epoch-based resource managers also performed well without huge overhead(?\%)\kk{TODO. fill out overhead}.
