%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Introduction}
\seclabel{intro}

%1. new hw trend has led to new systems
Modern systems with large main memories and massively parallel processors have inspired a new breed of high-performance memory-optimized OLTP systems \cite{Kallman+08,PandisJHA10,KemperN11,LarsonBDFPZ11,TuZKLM13}. These systems leverage spacious main memory to fit the whole working set in DRAM with streamlined, memory-friendly data structures. Further, optimizations for multicore and multi-socket hardware allow a much higher level of parallelism compared to conventional database systems. With disk overheads and delays removed, transaction latencies drop precipitously and worker threads can usually execute transactions to completion without interruption. The result is a welcome reduction in contention at the logical level and less pressure on whatever concurrency control (CC) scheme might be in place. A not very welcome result is an increasing pressure for scalable data structures and algorithms to cope with the increasing number of worker threads that concurrently execute transactions and need to communicate.

\labeledfigurewide{fig-write-ratio}{Performance of a memory-efficient OLTP engine with lightweight optimistic concurrency control, as the ratio of writes increases (left); and as the size of the database decreases (right).}

\vspace{2mm}
{\bf Interactions at the logical level.} 
%2. how current workloads make cc important again.
Many designs exploit the reduction in the pressure on CC, by employing very optimistic and lightweight schemes, boosting even further the performance of these systems on suitable workloads.
But, as is usually the case, it appears that database workloads stand ready to absorb any and all concurrency gains the memory-optimized systems have to offer. In particular, there is high demand for database systems that can handily serve database workloads that evolve increasingly heterogeneous, blending the gap between transaction and analytical processing. This trend is at least partly enabled by the improved concurrency and reduced contention offered by memory-optimized systems \cite{Farber+12}. Mixed workloads have two significant impacts on CC, however. First, the write/read ratio decreases from 1:2 (e.g. TPC-C) to 1:10 or less (e.g. TPC-E \cite{Chen+10,TozunPKJA13}), usually {\it by increasing the number of reads as the number of writes remains stable}. 
Second, workloads frequently include some fraction of large transactions that are {\it read-mostly rather than read-only}---a trend reflected in the TPC-E benchmark. Unfortunately, both of these workload properties result to an increase in effective concurrency control footprints, adding pressure to the CC scheme. 
Therefore, going forward and as the industry shifts to heterogeneous workloads served by memory-optimized engines, it is vital for them to employ effective and robust CC schemes. 

%3. current schemes: 2PL blocks + deadlock issues. existing schemes in new systems (mostly OCC) suffer (1) long tx with other tx going on can't commit (2) write clobber read
We observe that the CC schemes currently in vogue with memory-optimized system are not robust under contention, particularly when short write-intensive transactions coexist with longer read-mostly transactions.
For example, the two main families of approaches can be loosely classified as two-phase locking (2PL) and optimistic concurrency control (OCC). 2PL is common in traditional disk-oriented systems, and is often criticized because of high overheads, its policy of blocking transactions (leading to deadlocks and other scheduling problems), and a tendency to ``lock up'' (performance crash) once the aggregate transactional footprint grows too large, a state quickly attained when large transactions enter the system. OCC, on the other hand, never blocks readers---and may not even block writers---thus avoiding most scheduling issues. Although they differ in details, the rising generation of memory-optimized systems almost universally adopts a form of OCC that is effectively single-versioned, with read footprint validation at pre-commit.  Two systems that characteristically employ this type of OCC are Microsoft's Hekaton \cite{LarsonBDFPZ11} and Silo \cite{TuZKLM13}. This type of approach suffers badly in high-parallelism systems \cite{YuBPDS14} because transactions must abort if any portion of their read footprint is overwritten before they commit. 
In \figref{fig-write-ratio} we demonstrate how the performance of Silo, a representative of the camp of transaction processing engines with lightweight OCC, degrades as transactions have larger read footprint or when contention increases. (\secref{eval:setup} has details about the experimental setup.) \figref{fig-write-ratio}(left) shows that it just takes 0.1\% or 1\% of the touched records to be updates for the transaction throughput to drastically drop. While \figref{fig-write-ratio}(right) shows that the abort rate grows quickly as the same number of threads operate on smaller TPC-C databases, thereby on higher contention.

\vspace{2mm}
{\bf Interactions at the physical level.} 
But it is not only the interaction at the logical level that should be central to the design of a memory-optimized transaction processing engine. As commodity server hardware becomes increasingly parallel~\footnote{Note that the upcoming generation of Intel server-grade processor, Haswell-EP, comes with up to 18 cores (and 36 hyperthreads) per socket.} many of the low-level issues (latching, thread scheduling, etc..) and design decisions---at the architecture level---need to be revisited. The form of logging used, the storage management architecture, and scheduling policies for worker threads can impose drastic constraints on which forms of CC can be implemented at all, let alone efficiently. 
Therefore, it is difficult or impossible to adopt a different CC scheme without significant changes to the rest of the system. 
For example, it was reported in \cite{PortsG12} that the implementation effort required to add support for serializable snapshot isolation (SSI) in Postgres was very high. 
The point is not that such design choices should be avoided, but rather that they should be made only with a full awareness of the consequences for concurrency control. 

\vspace{2mm}
{\bf Physical partitioning.} Some systems sidestep the issues of logical and physical contention as well as the accompanying implementation complexity entirely by adopting physical partitioning and a single-threaded transaction execution model \cite{Kallman+08,KemperN11}. This execution model introduces a different set problems for mixed workloads and for workloads that are inherently difficult to partition.  Given the developments in scaling-out the performance of distributed OLTP systems, especially for easy-to-partition workloads, e.g. \cite{Corbett+12,BailisFHGS14,ThomsonA10}, as well as for high availability and cost-effectiveness reasons, we predict that the successful architectures will combine scale-out solutions build on top of non-partitioning-based scale-up engines within each node.
Therefore, we focus on the performance of non-partitioning-based memory-optimized engines within a single node.

\vspace{2mm}
{\bf ERMIA.} 
In \secref{desired} we are laying out the design principles that we believe are critical for transaction processing engines in the environment of highly-parallel servers with ample main memory. Next, on \secref{design}, we are presenting {\em ERMIA}, a memory-optimized transaction processing architecture that by combining epoch-based resource management and the indirection array technique \cite{SadoghiRCB13}, provides more robust CC, scalable thread interactions and easy recovery.  
\secref{eval} compares the performance of an ERMIA prototype against a representative of the new breed of memory-optimized shared-everything transaction processing systems, and shows how the resulting architecture does not necessarily sacrifice performance in other areas.


\section{Design directions}
\seclabel{desired}

In this section we briefly discuss desired properties of transaction processing system architectures. We focus on three areas: the concurrency control mechanism that determines the interaction between concurrent transactions at the logical level; the mechanism that controls the interaction/communication of threads at the physical level; and recovery. As we already argued in \secref{intro}, we are aiming for a scalable single-node design that relies as little as possible to physical partitioning.  

\vspace{2mm} 
{\bf Concurrency control:} 
Broadly speaking, there are two camps of CC methods: the pessimistic, e.g. two-phase locking (2PL), and the optimistic (OCC). As it has been shown in the past, e.g. \cite{AgrawalCL87}, in theory in presence of contention the  pessimistic methods beat optimistic if the overhead of those pessimistic methods is comparable with the overhead of the optimistic counterparts. However this is not easy to achieve. For example, a study of the disk-oriented SHORE storage manager estimates that there is at a 25\% overhead for locking-based pessimistic methods \cite{HarizopoulosAMS08}. In a memory-optimized system that has leaner codepaths, one could safely predict that the overhead of 2PL may be even larger. That's is a lot of slack for OCC to outperform pessimistic.

Having said that, typical memory-optimized engines that employ lightweight OCC and running on modern commodity servers, already provide quite high performance. Especially in the workloads they are optimized for (short-running partitionable transactions with small read and write footprints). But their performance is quite unpredictable once footprints grow larger and/or the ratio of writes increases. Therefore, the designer of such systems may seriously consider taking the hit and losing some of the peak performance by employing pessimistic CC in order to provide more robust behavior. In other words, it may be ok to lose say 15-20\% of the peak performance in some workloads if that measures in millions or hundreds of thousands of transactions per second, in order to be able to handle a wider spectrum of transactional and mixed workloads.

There are different flavor of optimistic, or opportunistic, CC. Many recent systems adopt a lightweight validation step at the end of the transaction, during pre-commit. Such kind of validation is very opportunistic, it is not robust, leaving the system vulnerable in many workloads.
Regardless optimistic or pessimistic, the CC mechanism should not only have a low false positive rate detecting conflicts, but it should allow the system to detect the glaring conflict cases (cases where a transaction is destined to fail) early enough and not at a later stage, such as during pre-commit.
Also, if a conflict is detected, either at pre-commit, or hopefully earlier, then blind retries without any guarantee about the success of the transaction at this time around waste useful cycles and create problems \cite{PortsG12}. In other words, the system should avoid repeatedly hitting the same conflicts. Instead, safe retries are desirable. 

At least to our knowledge, there is no (publicly available) system that is both fast enough and has the appropriate infrastructure to support the implementation of robust CC schemes. The infrastructure matters terribly. It decides whether it is even possible to implement a particular CC scheme, and whether that would be practical. For example, the effort to enhance Postgres with serializable snapshot isolation (SSI) required a very large implementation effort, since the team had to integrate what it is essentially a lock manager~\footnote{Once all this groundwork was done, extending Postgres to other CCs is relatively easy.}. And even then, the achieved performance was not impressive.  Many of the design decisions described in \secref{design} were specifically taken in order not to limit the implementation of CC schemes in some way.

\vspace{2mm}
{\bf Physical layer:} 
The interactions at the physical level are typically handled by a low-level component of any transaction processing system called the storage manager.
The implementation of the storage manager is tightly-coupled to the CC scheme and that makes it difficult to modify or extend the CC of legacy systems.  

In our opinion, one promising technique that provides desirable properties is the indirection array \cite{SadoghiRCB13,Diaconu+13}.
Indirection arrays are suitable for the physical implementation of CC for truly multi-versioned systems, as a single compare-and-swap (CAS) operation installs a new version of an object. 
As a comparison, Silo does not employ indirection arrays but instead it performs in place updates. For all practical purposes in Silo there is effectively a single committed version of any object with perhaps a private copy. So is Hekaton. Both systems are multi-versioned only for read-only transactions.

In some sense the indirection array is a lightweight bufferpool. With indirection arrays there is less chatter into the log. An update physically updates in place only the corresponding entry in the array. Otherwise, an update may cause several cascading changes. 
Updates do not require updating every reference to that object, say from secondary indexes. 
For example, with indirection arrays even the anti-caching technique \cite{DeBrabantPTSZ13} is largely simplified. That is, in the vanilla anti-caching algorithm all the secondary indexes have to be updated, whenever a record is evicted from memory. The corresponding overhead increase with the number of secondary indexes on the table. With the indirection array there is no need for more than one update. 
There are also some low-level reasons, detailed in \secref{design:oid}, for using indirection arrays. For example, space management becomes easier, as it is simple to implement cache-friendly compact index structures, such as CSB trees \cite{RaoR00}, as long as we can tolerate the extra level of indirection. Since for other reasons mentioned earlier, we are inclined to employ indirection array, those benefits come from free. 

The flexibility in implementing various CC schemes is greatly enhanced if we can establish total ordering. One way to achieve total ordering, is through a centralized log, which can be used for establishing the transaction commit order. Therefore we opt using a centralized log, but we minimize the interaction with it to only a single communication per transaction. 
In contrast, Silo employs a epoch-based ordering that is only partial. 

\vspace{2mm}
{\bf Recovery:} 
System designers should also treat recovery as a first class citizen when designing a transaction processing system. Many of the recent proposals do not provide thorough design for recovery, at least not even close to the level of detail of \cite{MohanHLPS92}.   For example, Silo's current codebase does not even implement recovery, and VoltDB uses log shipping. 

We argue in favor of a centralized log facility, where transactions have minimal interaction with it, ideally once per transaction commit through a compare-and-swap operation.  
The combination of indirection arrays with a centralized logging facility drastically simplifies recovery. There is no need for undo at all, and redo is trivial, as it just needs log analysis pass to restore in-memory data structures. 

