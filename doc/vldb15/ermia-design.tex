%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{ERMIA: Fast main-memory OLTP with robust concurrency control}
\seclabel{design}

In this section we describe the key pieces of ERMIA, with a focus on why we choose the design trade-offs we do.

\subsection{Logging}
The log manager is a pivotal component in most database engines. It provides---if anything does---a centralized point of coordination that other pieces of the system build off of and depend on. ERMIA's log manager generates a commit log sequence number (LSN) for the transaction and reserves log buffer space for the transaction's log records with a \textit{single global atomic operation}. Achieving this required two key insights: first, the transaction can combine its log records into large blocks, avoiding the redundancy of writing individual log record headers and reducing the number of trips to the log. Second, the LSN space need not be contiguous as long as we can still convert easily between an LSN and the corresponding disk address.

The first property arises from our use of append-only storage. We achieve the second property by assigning each LSN to a ``segment'' and storing its segment number in the low order bits; the LSN's position on disk can be determined by looking up, and subtracting off, its segment's starting offset (read-only). Transactions race to ``open'' a new segment if they obtain an LSN past the end of the current one. Unlucky transactions holding an LSN in the gap between two segments can simply discard it and request a new one. Segments can be as large as 100GB or more, however, so overflows will be rare. Once an LSN and segment have been assigned, the transaction verifies availability of space in the log's circular memory buffer (again, read-only); only in case the buffer is full will transactions have to block pending space, but disk arrays can readily absorb the sequential write-only I/O stream.

An additional feature of the log is that transactions acquire a commit LSN before entering pre-commit, allowing validation of multiple transactions to proceed smoothly in parallel; depending on the outcome of pre-commit, a transaction either writes its log entries or a skip record (to abort) to the reserved log block.

Finally, because the shared counter is implemented as a wait-free linked list, the transaction can notify the log writer that its block is ready to be flushed by simply flagging its node as ``dead'' (a blind store). The log writer periodically scans the list and writes out all log blocks that precede the oldest ``live'' buffer allocation; transactions do not touch each others' nodes, and the log writer only reads them and flags dead nodes for the garbage collector.

\subsection{Epoch-based resource management}
We have developed a lightweight epoch management system that can track multiple timelines of differing granularities in parallel. A multi-transaction-scale epoch manager implements garbage collection of dead versions and deleted records, a medium-scale epoch manager implements read-copy-update (RCU) that manages physical memory and data structure usages~\cite{McKenneyS98}, and a very short timescale epoch manager tracks transaction IDs (TIDs), which we recycle aggressively (see details in \secref{tm}).

The key to efficiency here is to avoid flagging stragglers unless it is absolutely necessary (because coordinating with non-responsive threads is very expensive). Therefore, ERMIA does not attempt to reclaim resources for epoch $N$ until epoch $N+2$ begins. This way, potential stragglers have all of epoch $N+1$ to quiesce without penalty; however, epoch $N+3$ cannot begin until the last straggler from epoch $N$ completes. This four-phase scheme communicates far less with stragglers than the traditional two-phase \tianzheng{three-phase?} approach while maintaining the same worst-case timing bound. It allows us to track epochs at a very fine granularity when necessary. 

\subsection{Transaction management}
\seclabel{tm}
Each transactions in the system is assigned a slot in a global transaction state table when it begins. This fixed-size table holds the transaction's begin time (which is the log's end LSN at the time it started), status, and end time (if applicable). TIDs are a combination of table offset and epoch, with an epoch manager to prevent entries from being recycled too soon. Update transactions write their TIDs into each version they create, change their status to pre-commit, acquire a commit LSN (or are given one by an impatient peer), and finally commit atomically by changing their status to ``committed.'' A post-commit cleanup step involves replacing the transaction's TID with its commit LSN, at which point the state table entry is no longer needed and can be recycled by the epoch manager. Other transactions that encounter a TID in a version can reliably verify its commit status and age by visiting the transaction state table, and---if necessary---will help a peer enter pre-commit by acquiring a log block on its behalf.

\subsection{Indirection arrays}
\seclabel{design:oid}

The indirection arrays used in ERMIA are very similar to the ones proposed in the literature. All logical objects are identified by an object ID (OID) that maps to a slot in an OID array that contains the physical pointer to data. The pointer may reference disk, or a chain of versions stored in memory. As with Hekaton, uncommitted versions are never written to disk; but unlike Hekaton, we dispense with delta records (too expensive to apply) and use pure copy-on-write. New versions can be installed by an atomic compare-and-swap operation, and an uncommitted record at the head of the chain constitutes a write lock for CC schemes that care to track write-write conflicts (as most do). 

\subsection{Concurrency control}
\seclabel{design:cc}

ERMIA has been designed from the ground up to allow efficient implementations of a variety of CC mechanisms. It can use read set validation (like Silo and Hekaton), but can also provide snapshot isolation to writers.% and even efficient implementations of serializable snapshot isolation~\cite{Cahill08RF}.
%We are also in the process of developing new CC schemes which promise lower abort rates than SSI with lower overhead and reduced implementation complexity.
The other components in the system work together to make efficient CC possible: cheap (but optional) multi-versioning allowed by the indirection arrays, the total commit ordering afforded by the log, and the ability to determine easily the age of a version thanks to the transaction manager. The first (MVCC via indirection array) is available in other systems, but the other two are key enablers of %the new CC schemes we are developing.
an enhanced version of serializable snapshot isolation~\cite{Cahill08RF} that we have implemented in ERMIA for serializability.

\subsection{Recovery}
\seclabel{design:recovery}

Recovery in ERMIA is straightforward because the log contains only committed work. OID arrays are the only real source of complexity, as they are volatile in-memory data structures that make it possible to find all other objects in the system. Logical objects (records) are physically logged, while physical data (allocator state and OID array contents) use logical logging. OID arrays are themselves objects stored in a master OID array, but they are updated in place to avoid overloading the log, with changes replayed by a log analysis step that reads only log block headers. This analysis step is very fast, because the skipped-over log payloads account for 90\% or more of the total log. In order to support efficient recovery, system transactions occasionally checkpoint the OID arrays using a fuzzy checkpointing mechanism to minimize the impact on user transactions. Because the log is the database, recovery only needs to rebuild the OID arrays in memory; anti-caching will take care of loading the actual data, though background pre-loading is highly recommended to minimize cold start effects.

%\subsection{Prototype Implementation}
%\seclabel{design:prototype}
%
%We implement a prototype of the ERMIA architecture and measure its performance.  For the implementation of the prototype we use a large fraction of the publicly available Silo codebase~\footnote{Silo's codebase can be downloaded from: https://github.com/stephentu/silo.}.  Silo uses the Masstree \cite{MaoKM12} as a cache-efficient index structure. 
