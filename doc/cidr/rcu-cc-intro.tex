%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Introduction}
\seclabel{intro}

%1. new hw trend has led to new systems
Modern systems with large main memories and massively parallel processors have inspired a new breed of high-performance memory-optimized OLTP systems \cite{Kallman+08,PandisJHA10,KemperN11,LarsonBDFPZ11,TuZKLM13}. These systems leverage spacious main memory to fit the whole working set in DRAM with streamlined, memory-friendly data structures. Further, optimizations for multicore and multi-socket hardware allow a much higher level of parallelism compared to conventional database systems. With disk overheads and delays removed, transaction latencies drop precipitously and worker threads can usually execute transactions to completion without interruption. The result is a welcome reduction in contention at the logical level and less pressure on whatever concurrency control (CC) scheme might be in place. A not very welcome result is an increasing pressure for scalable data structures and algorithms to cope with the increasing number of worker threads that concurrently execute transactions and need to communicate.

\labeledfigurewide{fig-write-ratio}{Performance of a memory-efficient OLTP engine with lightweight concurrency control mechanism, as the ratio of writes increases (left); and as the size of the database decreases (right).}

\vspace{2mm}
{\bf Interactions at the logical level.} 
%2. how current workloads make cc important again.
Many designs exploit the reduction in the pressure on CC, by employing very optimistic and lightweight schemes, boosting even further the performance of these systems on suitable workloads.
But, as is usually the case, it appears that database workloads stand ready to absorb any and all concurrency gains the memory-optimized systems have to offer. In particular, there is high demand for database systems that can handily serve database workloads that evolve increasingly heterogeneous, blending the gap between transaction and analytical processing. This trend is at least partly enabled by the improved concurrency and reduced contention offered by memory-optimized systems \cite{Farber+12}. Mixed workloads have two significant impacts on CC, however. First, the write/read ratio decreases from 1:2 (e.g. TPC-C) to 1:10 or less (e.g. TPC-E \cite{Chen+10,TozunPKJA13}), usually {\it by increasing the number of reads as the number of writes remains stable}. 
Second, workloads frequently include some fraction of large transactions that are {\it read-mostly rather than read-only}---a trend reflected in the TPC-E benchmark. Unfortunately, both of these workload properties result to an increase in effective concurrency control footprints, adding pressure to the CC scheme. 
Therefore, going forward and as the industry shifts to heterogeneous workloads served by memory-optimized engines, it is vital for them to employ effective and robust CC schemes. 

%3. current schemes: 2PL blocks + deadlock issues. existing schemes in new systems (mostly OCC) suffer (1) long tx with other tx going on can't commit (2) write clobber read
We observe that the CC schemes currently in vogue with memory-optimized system are not robust under contention, particularly when short write-intensive transactions coexist with longer read-mostly transactions.
For example, the two main families of approaches can be loosely classified as two-phase locking (2PL) and optimistic concurrency control (OCC). 2PL is common in traditional disk-oriented systems, and is often criticized because of high overheads, its policy of blocking transactions (leading to deadlocks and other scheduling problems), and a tendency to ``lock up'' (performance crash) once the aggregate transactional footprint grows too large, a state quickly attained when large transactions enter the system. OCC, on the other hand, never blocks readers---and may not even block writers---thus avoiding most scheduling issues. Although they differ in details, the rising generation of memory-optimized systems almost universally adopts a form of OCC that is effectively single-versioned, with read footprint validation at pre-commit.  Two systems that characteristically employ this type of OCC are Microsoft's Hekaton \cite{LarsonBDFPZ11} and Silo \cite{TuZKLM13}. This type of approach suffers badly in high-parallelism systems \cite{YuBPDS14} because transactions must abort if any portion of their read footprint is overwritten before they commit. 
In \figref{fig-write-ratio} we demonstrate how the performance of Silo, a representative of the camp of transaction processing engines with lightweight OCC, degrades as transactions have larger read footprint or when contention increases. (\secref{eval:setup} has details about the experimental setup.) \figref{fig-write-ratio}(left) shows that it just takes 0.1\% or 1\% of the touched records to be updates for the transaction throughput to drastically drop. While \figref{fig-write-ratio}(right) shows that the abort rate grows quickly as the same number of threads operate on smaller TPC-C databases, thereby on higher contention.

\vspace{2mm}
{\bf Interactions at the physical level.} 
As commodity server hardware becomes increasingly parallel~\footnote{Note that the upcoming generation of Intel server-grade processor, Haswell-EP, comes with up to 18 cores (and 36 hyperthreads) per socket.} many of the low-level issues (latching, thread scheduling, etc..) and design decisions---at the architecture level---need to be revisited. The form of logging used, the storage management architecture, and scheduling policies for worker threads can impose drastic constraints on which forms of CC can be implemented at all, let alone efficiently. 
Therefore, it is difficult or impossible to adopt a different CC scheme without significant changes to the rest of the system. 
For example, it was reported in \cite{PortsG12} that the implementation effort require to add support for SSI in Postgres was very high. 
The point is not that such design choices should be avoided, but rather that they should be made only with a full awareness of the consequences for concurrency control. 

\vspace{2mm}
{\bf Partitioning.} Some systems sidestep the issues of logical and physical contention as well as the accompanying implementation complexity entirely by adopting physical partitioning and a single-threaded transaction execution model \cite{Kallman+08,KemperN11}. But that introduces a different set problems for mixed workloads and for workloads that are inherently difficult to partition.  Given the developments in scaling-out the performance of distributed OLTP systems, especially for easy-to-partition workloads, e.g. \cite{Corbett+12,BailisFHGS14,ThomsonA10}, as well as for high availability and cost-effectiveness reasons, we predict that the successful architectures will combine scale-out solutions build on top of non-partitioning-based scale-up engines within each node.

\vspace{2mm}
{\bf ERMIA.} 
In \secref{desired} we are laying out the design principles that we believe are critical for transaction processing engines in the environment of highly-parallel servers with ample main memory. Next, on \secref{design}, we are presenting {\em ERMIA}, a memory-optimized transaction processing architecture that by combining epoch-based resource management and the indirection array technique \cite{SadoghiRCB13}, provides more robust CC, scalable thread interactions and easy recovery.  
\secref{eval} compares the performance of an ERMIA prototype against a representative of the new breed of memory-optimized shared-everything transaction processing systems, and shows how the resulting architecture does not necessarily sacrifice performance in other areas.


\section{Design directions}
\seclabel{desired}

In this section we briefly discuss our desired properties of a transaction processing system architecture. We primarily focus on three areas: the concurrency control mechanism that determines the interaction between concurrent transactions at the logical level; the mechanism that controls the interaction/communication of threads at the physical level; and recovery. As we already argued in \secref{intro}, we are aiming for a scalable single-node design that relies as little as possible to physical partitioning.  

\vspace{2mm} 
{\bf Concurrency control:} 
Broadly speaking, there are two camps of CC methods: the pessimistic, e.g. two-phase locking (2PL), and the optimistic (OCC). As it has been shown in the past, e.g. \cite{AgrawalCL87}, in theory in presence of contention the  pessimistic methods beat optimistic if the overhead of those pessimistic methods is comparable with the overhead of the optimistic counterparts. However this is not easy to achieve. For example, a study of the SHORE storage manager estimates that there is at a 25\% overhead for locking-based pessimistic methods \cite{HarizopoulosAMS08}. That's is a lot of slack for OCC to outperform pessimistic.

Having said that, typical memory-optimized engines that employ lightweight OCC and running on modern commodity servers, already provide quite high performance. Especially in the workloads they are optimized for (short-running partitionable transactions with small read and write footprints). Given that pessimistic CC is way more robust, the designer may seriously consider taking the hit and losing some of the peak performance in order to provide more robust behavior. In other words, it may be ok to lose say 15-20\% of the peak performance if that measures in millions or hundreds of thousands of transactions per sec.

Ideally the CC mechanism should not only have a low false positive in rate detecting conflicts, but it should allow the system to detect the glaring conflict cases (cases where a transaction is destined to fail) as early enough and not during pre-commit.
There are different flavor of optimistic, or opportunistic, CC. Many recent systems adopt a lightweight validation step at the end of the transaction, during pre-commit. Such kind of validation is very opportunistic, it is not robust, leaving the system vulnerable in many workloads.
Also, if a conflict is detected, either at pre-commit, or hopefully earlier, then blind retries without any guarantee about the success of the transaction at this time around waste useful cycles and create problems \cite{PortsG12}. In other words, the system should avoid repeatedly hitting the same conflicts. Instead, safe retries are desirable. 

At least to our knowledge, there are no (publicly available) systems, that are both fast enough and have the appropriate infrastructure to support the implementation of proper, robust CC schemes. The infrastructure matters terribly. It decides whether it is even possible to implement a particular CC scheme, and whether that would be practical. For example, the effort to enhance Postgres with serializable snapshot isolation (SSI) required a very large implementation effort, since the team had to integrate what it is essentially a lock manager~\footnote{    Once all this groundwork was done, extending Postgres to other CCs was relatively easy.}. And even then, the achieved performance was not impressive.  Many of the design decisions described in \secref{design} were specifically taken in order not to limit the implementation of CC schemes in some way.

\vspace{2mm}
{\bf Interactions at the physical level:} 
- those interactions are typically handled by a low-level component of the system called the storage manager. The storage engine
At the physical layer, we need a storage management system to build on top of and there are not systems that provide the properties
- indirection array (Mohammed) 
  - benefiting of the observations of both
  - anti-caching
  - gives us many desirable properties wrt to physical layer inderaction between threads:
  - as Mo pointed: you have less chatter in to the log, you update your entry and that's it, otherwise you may have updates that go up to the root. Not having to update every reference
    - silo updates in place, it is effectively a single committed version with a private copy system for all practical purposes. so is Hekaton.
    - it is MVCC only for read-only xct, safe snapshots
    - read footprint validation
  - easier physical implementation of CC for multi-versioned systems, a single CAS installs a new version
  - anti-caching becomes easier! e.g. in the vanilla anti-caching algorithm all the auxilliary indexes had to be updated, whenever a record was evicted from memory, with the indirection array, only one place has to be updated.
    - essentially a much lighter bufferpool
  - kind of esoteric reason detailed in \secref{design:oid}: space management becomes easier, as it becomes easier to implement cache-friendly compact index structures, such as CSBs, as long as we can tolerate the extra dereference -- those come from free. Nice side-benefit.
- no partial orders, SILO had partial orders, the central log establishes the commit order
  - recovery becomes drastically simplified, no need for undo at all, redo is trivial just need log analysis pass to restore in-memory data structures.
  - need checkpoint plus log analysis to recover!

{\bf Recovery:} 
-- Recovery has also to be a first class citizen
- having a log that requires just one central communication, through a CAS
- lots of hand-wavvy claims
- you need to have to recovery story shorted out before you start
- voltdb -- does log shipping
- silo -- does not even implement recovery
- hekaton -- hand-wavvy description

