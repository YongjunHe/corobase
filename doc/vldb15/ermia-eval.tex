%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Evaluation}
\seclabel{eval}
In this section, we compare the performance of ERMIA with Silo, a representative of lightweight OCC camp, from two perspectives: 1)CC impact on performance and 2)scalability of underlying storage manager. 
The purposes of the performance evaluation are the followings. First, we want to show that the recent proposals for main-memory OLTP solutions with lightweight optimistic concurrency control schemes can perform well and are suitable for a limited subset of transactional applications, primarily due to limitations imposed by their (tightly-integrated) concurrency control mechanism. Second, the proposed ERMIA design offers a more efficient concurrency control, which allows its performance to remain high under contentious workloads. At the same time, ERMIA does not suffer significant sacrifices in performance, even in workloads where the specialized solutions shine. In addition to the CC impact on performance, we also show that ERMIA achieves multicore scalability thanks to scalable storage manager, mainly achieved by log manager, epoch-based resource manager and latch-free indirection array. 


\subsection{Experimental setup}
\seclabel{eval:setup}
% System
We used a 4-socket server with 6-core Intel Xeon E7-4807 processors, for a total of 24 physical cores and 48 hyperthreads. The machine has 64GB of RAM. All worker threads were pinned to a dedicated core and NUMA node to minimize context switch penalty and inter-socket communication costs. Log records are written to \textit{tmpfs} asynchronously.

\subsection{TPC-E}
\seclabel{eval:tpce}
We first explore how ERMIA and Silo react to contention in the TPC-E which is a new standard OLTP benchmark. Although the TPC-C has been dominantly used in OLTP benchmark so far, the TPC-E was designed as a more realistic OLTP benchmark with modern features. The TPC-E models brokerage firm activities and has more sophisticated schema model and transaction execution control. It is also known for higher read to write ratio than the TPC-C. 

%TPCE + contention
Since the TPC-E is not a heterogeneous workload, we introduce a new read-mostly analytic transaction that evaluates total assets of a random customer account group and inserts the analytic activity log into \textit{analytic\_history} table. Total assets of the accounts are computed by joining \textit{holding\_summary} and \textit{last\_trade} tables. The vast majority of contentions occur between the analytic transaction and \textit{trade-result} transaction. The TPC-E parameters are 5000 customers, 500 scale factor and 10 initial trading days. The analytic transaction takes 20\% out of the original TPC-E transaction mix. % TODO. new workload mix, footnote?

\labeledfigurewide{fig-tpce-robustness}{TPS of total transactions(left); TPS of analytic transaction (right) of TPCE-hybrid, varying contention degree}

% chart - Commit rates / Abort rates, varying contention degree. 
\figref{fig-tpce-robustness}(left) shows the throughput with 24 worker threads, varying contention degree when we run the TPC-E with the analytic transaction(TPCE-hybrid). To vary contention degree, we adjusted the size of a customer account group to be scanned by the analytic transaction, from 5\% to 20\%. In this figure, we can observe that Silo is more sensitive to contention; the performance gap gets larger as we increase contentions. In case of ERMIA-SSI, it falls behind the Silo by 26\% at 5\% contention degree, however, it starts to close the gap at higher contention level and catches up with Silo at 20\% contention degree. 
The main cause for the result is that the contention in the workload imposed heavy pressure on OCC protocol; OCC enforced transactions to abort even if a small fraction of their read-set is invalidated by updaters. Meanwhile, ERMIA endured the contention by avoiding read-write conflict with snapshot isolation and distributed the contentions over the snapshots. 
we now focus on the throughput of the analytic transactions to see how well both systems support analytics on transactional data. In \figref{fig-tpce-robustness}(right), we report the throughput of the analytic transactions only from the previous experiment. The analytic transactions were aborted massively even under small contention degree on Silo. At 5\% degree, Silo produces less than half analytic transaction commits to ERMIA-SI. It drops sharply as we increase contention degree. ERMIA-SSI also shows stable analytic transaction commit rate, even though its serializability cost dragged down overall throughput. To recap, ERMIA provides balanced query and transaction performance, while Silo extremely gives penalty to read-intensive long transactions. This is a negative implication for heterogeneous workload to run with OCC. 

% scalability 
\labeledfigurewide{fig-tpce-scalability}{Throughput when running TPCE-hybrid(left); TPCE-original(right)}
\labeledfigurewide{fig-tpcc-scalability}{Throughput when running TPCC-contention(left); TPCC-partitioned(right)}
In \figref{fig-tpce-scalability}(left), we fix contention degree to 20\% and see scalability trends, increasing the number of workers. Overwhelmed by contentions, Silo does not achieve linear scalability, even with its excellent scalability of the underlying physical layer. ERMIA benefits from its robust CC scheme and its storage manager did not forestall achieving linear scalability. This figure shows that not only the scalability of underlying physical layer, but also CC scheme's capability to deal with contention in logical level dictates overall performance. We also performed the same experiment in the original TPC-E, without the analytic transaction. As illustrated in \figref{fig-tpce-scalability}(right), both ERMIA-SI and Silo achieve linear scalability over 24 cores. Silo does not suffer from contention, since the TPCE-original has insignificant read-write contentions. ERMIA-SSI delivered 83\% of ERMIA-SI performance with 24 threads due to the serializability checking cost.

\subsection{TPC-C} 
\seclabel{eval:tpcc}
We also run the TPC-C where lightweight OCC can shine. This experiment will be focusing on evaluating storage manager's scalability naturally, as the TPC-C imposes little pressure on CC scheme. the TPC-C is well-known for update-heavy workload and small transaction footprints. Also, it is well-partitionable workload; conflicts can be reduced even further by partitioning and single-threading on the partition, setting the almost ideal environment for the lightweight OCC. The only source of contention under partitioned TPC-C is multi-partition transaction, however, its impact on CC is dampened by small transaction footprints, avoiding the majority of read-write conflicts. 

% TPCC-original
We measure throughput of both systems in the partitioned TPC-C(TPCC-partitioned), varying the number of worker threads. All worker threads are given a local warehouse and does not change the local warehouse during runtime. The percentage of cross partition transaction 1\%. \figref{fig-tpcc-scalability}(right) shows that both systems scale reasonably over 24 cores. Compared to Silo, ERMIA-SI and ERMIA-SSI fall behind Silo by 10\% and 20\% respectively at the peak performance for the following reasons. First, the TPC-C did not suppress OCC, as it is lack of contention. Second, Silo benefits from the TPC-C's small cache footprints. Unlike Silo, ERMIA incurs larger cache footprints due to multi-versioning. Specifically, it pays additional cache miss costs during traversing indirection array to find a visible version. This extra cost was invisible in the TPC-E because caching effect were canceled by large cache footprint in both systems. Thus, Silo outperforms ERMIA in workloads where 1) contention is rare and 2) transaction footprint is small enough to take advantage of cache-optimization; the TPC-C is the compelling example of such workload. Otherwise, ERMIA does not fall behind significantly. ERMIA-SSI had extra 10\% cost to guarantee serializability due to serial dependency checking. %TODO. detail?

% TPCC-random WH
We now enforce worker threads to pick a random working partition, following non-uniform distribution. The purpose of this modification is to bring reasonable amount of contentions to the TPC-C and to see how the previous results change. As shown in \figref{fig-tpcc-scalability}(right), we can see that Silo is more sensitive to contention than ERMIA-SI. Compared to TPCC-partitioned, the throughput of Silo decreased by approximately 30\%, while ERMIA delivered 15\% lower performance. As a consequency, ERMIA-SI and Silo show almost same performance.

