%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Evaluation}
\seclabel{eval}
In this section, we compare the performance of ERMIA with SILO, a representative of the lightweight OCC camp, from two perspectives: 1) CC impact on overall performance and 2) scalability of underlying storage manager. 
The purpose of performance evaluation are the followings. First, we want to show that the recent proposals for main-memory OLTP solutions with lightweight optimistic concurrency control schemes can perform well and are suitable for only on a limited subset of transactional applications, primarily due to limitations imposed by their (tightly-integrated) concurrency control mechanism. Second, the proposed ERMIA design offers a more efficient concurrency control, which allows its performance to remain high for a larger spectrum of workloads even under contention. At the same time, ERMIA does not suffer significant sacrifices in performance, even in workloads where the specialized solutions shine. Lastly, we will measure how much two CC schemes, SI and SSI, cost quantitatively. In addition to the CC impact on performance, we also show that ERMIA achieves multicore scalability thanks to scalable storage manager, mainly achieved by log manager, epoch resource manager and latch-free indirection array. 


\subsection{Experimental setup}
\seclabel{eval:setup}
% System
We used a 4-socket server with 6-core Intel Xeon E7-4807 processors, for a total of 24 physical cores and 48 hyperthreads. The machine has 64GB of RAM. We gave pre-faulted memory pool to both systems to avoid excessive page faults during runtime. The maximum number of worker threads are same with the number of physical cores in the machine because it is enough to saturate CPUs in main-memory databases and the more workers contend for CPU, interferring each other. All worker threads were pinned to a dedicated core and NUMA node to minimize context switch penalty and inter-socket communication costs. Log records are written to tmpfs asynchronously and we used TCmalloc for memory allocation.

\subsection{TPC-E}
\seclabel{eval:tpce}
We first explore how ERMIA and SILO react to contention in the TPC-E which is a new standard OLTP benchmark. Although TPC-C has been dominantly used to evaluate OLTP systems performance for the past few decades, TPC-E was designed as a more realistic OLTP benchmark with modern features. TPC-E models brokerage firm activities and has more sophisticated schema model and transaction execution control. It is also known for higher read to write ratio than TPC-C. 

%TPCE + contention
However, we found that there are no significant contentions in TPC-E, as the most read-intensive transactions do not access to contentious tables heavily and heavy use of secondary indicies can reduce transaction footprint even further. Since TPC-E is not designed to be a heterogeneous workload, we introduced a new long query into original workload mix. The long query scans through random range of customer account table and the customer account's holding summary randomly. Trade-result transaction is the main updater of the tables. Hence, the majority of contentions occur between the long query and the trade-result transaction. 
%TODO. new query with SI anomaly 
The parameters we set are 5000 customers, 500 scale factor and 10 initial trading days.(The initial trading day parameter was limited by our machine's memory capacity). %We also replicate TPC-E's market emulator(MEE) and make it thread-local structure to avoid MEE's internal locking overhead. 

\labeledfigurewide{fig-tpce-scalability}{ scalability: tpce-contention (left); tpce-original(right) }
\labeledfigurewide{fig-tpcc-scalability}{ scalability: tpcc-contention (left); tpcc-original(right) }
\labeledfigurewide{fig-tpce-robustness-commits}{ robustness }

% chart - Commit rates / Abort rates, varying contention degree. 
To vary contention degree, we tune long query's access range size, rather than changing transaction mix. The scan range varies from 5\% to 20\%. \ref{table:workload-mix} describes the workload mix we used. % workload mix table? or footnote? 
\figref{fig-tpce-scalability}(left) shows commit rates with 24 worker threads under varying contention degree. The X-axis indicates the long query access range size(the scan range with customer account table is chosen randomly). Two CC schemes, SI and SSI, are evaluated on ERMIA. As shown in the figure, SILO is slightly faster than ERMIA-SI at the small(5\%) contention. As we increase contention degree, we can observe that ERMIA starts to outperform SILO and the performance gap gets larger; ERMIA has double commit rates along with half aborts at the 10\% query range. %ERMIA-SSI delivers ??\% higher throughput than SILO, even with the expensive serializability guarantee cost.
The main cause for the result is that the contention in the workload imposed heavy pressure on the SILO's OCC protocol, while ERMIA distributed the contention across multiple versions. As a result, most long queries were not able to protect their read sets from updaters' aggressive invalidation. SI protected the transactions' read-sets from updaters effectively. OCC's lazy commit decision making also partly caused low performance under high contention. It decides commit/abort at precommit time and hands over CPU to the next transaction, if aborted. Thus, the more aborts, the more waste of CPU time for transactions to be aborted in OCC.

% Long query throughput
We take a look at long query commit rates in the \ref{fig:tpce-robust-query}. ERMIA-SI produces query commits more than ten times than SILO at 20\% scan range. %TODO. put SSI number also.
The long queries on SILO were shoot down massively even under small contention. It is obviously negative implication for heterogenous workload to run on the OCC, even if it delivers reasonable overall throughput. ERMIA provides balanced query/transaction performance, while SILO extremely favors updaters.

% scalability 
In \ref{fig:tpce-contention-scalability}, we fix contention degree to 20\% scan range and see scalability trends, increasing the number of workers. Overwhelmed by contention, SILO does not achieve scalability, even with its excellent scalability of underlying physical storage manager. ERMIA benefits from its robust CC scheme and its storage manager did not forestall achieving linear scalability. This figure shows that CC scheme's capability to deal with contention dictates overall performance in logical level, no matter how the underlying physical layer is scalable, under heterogeneous workload. We also performed same experiments in original TPC-E, without long query. %In, \ref{fig:tpce-org-scalability}, we see lack of contentions did not take down SILO and both scalable storage managers scale reasonably over 24 cores. Both ERMIA-SI and ERMIA-SSI delivered slightly lower peak performance. % not sure we need to include tpce-original.

\subsection{TPC-C} 
\seclabel{eval:tpcc}
We also run TPC-C where lightweight OCC camp shine. This experiment will be focusing on evaluating storage manager's scalability, as TPC-C impose little pressure on CC scheme. TPC-C is well-known for update-heavy workload and small transaction footprints. Also, it is well-partitionable workload; conflicts can be reduced even further by partitioning and single-threadinging on the partition, setting ideal environment for the lightweight OCC. The only source of contention under partitioned TPC-C is multi-partition transaction, however, its impact on CC is dampened by small transaction footprints, avoiding the majority of read-write conflicts. % cross-partition transaction ratio, partition lock, 

% TPCC-original
We measure throughput of both systems in TPC-C, varying the number of worker threads. All worker threads are given home warehouse and does not change the home warehouse during runtime. Cross partition transactinos take ??\%. \ref{fig:tpcc-org-scalability} shows that both systems scale reasonably over 24 cores. Compared to SILO, ERMIA falls behind SILO by 20\% and ??\%, with SI and SSI respectively, at the peak performance for the following reasons. First, SILO has more lightweight physical layer as it goes by single-version store. Second, CC did not affect overall performance due to the lack of contention. Consequently, the lack of contention turned ERMIA's effort toward robustness into marginal overhead; the indirection array is at the center of this trade-off between TPC-C peak performance and robustness. The indirection array maintains snapshots to support CC scheme to distribute contentions. Whenver ERMIA accesses to database tuples, it has to chase pointers in the indirection array to find a visible version and incurs more cache footprints subsequently. ERMIA-SSI has extra cost to guarantee serializability; it keeps track of transaction dependency information and lookup them to identify presence of cyclic dependency. The overhead is "cost" to pursue our design goals and we believe that robustness is worthwhile to invest for wider range of applications, at the expense of insignificant performance loss in the specific workload.

% TPCC-random WH
This time, we change workers-warehouse binding. We enforced worker threads to pick a partition randomly, following non-uniform distribution, during runtime. The purpose of this modification is to bring reasonable amount of contentions in TPC-C and to see how the previous results change. We can see, in \ref{fig:tpcc-contention-scalability}, the performance gap gets smaller than in original TPC-C as SILO is affected by the contention again. Nevertheless, as we mentioned earlier, write-write conflict is the dominant contention type and transaction is quite short-lived in the TPC-C, therefore, the impact of contention was limited.  %TODO. numbers needed

\subsection{Performance study}
\seclabel{eval:perf-study}

profiling and performance analysis with breakdown(pie or bar chart will look intuitively)

\begin{itemize}
\item SI overhead - indirection array  -10\%
\item SSI overhead 
\item Index - the biggest overhead
\item log manager - sustains 24 core scalability. no need to give up total ordering which is a key to achieve SI.
\end{itemize}

While traversing the version chain, ERMIA has ?\% larger cache footprint and subsequent more cache misses than SILO. 

SSI overhead - serial checking overhead, phantom overhead. needs comparison with other SSI techniques.  -20\%

Scalability is achieved with various techniques collectively. Cache optimization, NUMA-aware memory management, epoch resource manager and latch-free indirection array help to achieve scalability. 

Log manager and total ordering overhead - marginal
Log manager was able to sustain 400K commits/s. As a key enabler of SI, log manager's cheap cost proves we don't need to give up total ordering for scalability. Actually, it enables robust CC which can affect performance more than scalability in some cases.
TID allocation and epoch manager's overhead was also negligible. 

