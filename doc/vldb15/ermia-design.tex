%% -*- tex-main-file:"ermia.tex" -*-

\section{ERMIA}
\seclabel{design}
In this section, we start with an overview of ERMIA, and then describe its key pieces, with a focus on why we choose the design trade-offs we do.

\subsection{Overview}
ERMIA is designed around epoch-based resource management and extremely efficient, centralized logging. \figref{ermia-arch} shows the major components in ERMIA and how they interact with each other. The log manager, TID manager, and garbage collector are three major entities that use the epoch-based resource management machinery. To avoid log buffer contention, each transaction acquires its private log buffer from the log manager. Upon commit, the transaction will fix its global order in the log via a single compare-and-swap instruction and flush all its log records in one go (see~\ref{subsec:logging} for details). Although both log buffers and TIDs are managed by the epoch-based resource management mechanism, they use separate epoch duration and can track stragglers efficiently. In ERMIA, transaction threads access the database through indexes. Different from traditional tree structures which give access to data in the leaf level, in each index we embed an indirection array, which can provide easy implementation of snapshot isolation and garbage collection. Indexed by object IDs (OIDs), each array entry points to a chain of historic version for the object (tuple). The garbage collector periodically goes over all version chains and remove superseded versions that are not needed by any transactions.
\labeledfigure{ermia-arch}{Architecture of ERMIA.}

\subsection{Logging}
\label{subsec:logging}
The log manager is a pivotal component in most database engines. It provides a centralized point of coordination that other pieces of the system build off of and depend on. Because of its central nature, the log is also a notorious source of contention in many systems. ERMIA's log manager retains the benefits of a serial ``history of the world'' while largely avoiding the contention issues that normally accompany it. Useful features of the log include:

\setcounter{mycounter}{0}
\begin{list}{\arabic{mycounter}.}{\usecounter{mycounter}\leftmargin=1.4em}
  \itemsep=0.1em
  \parsep=0em
  \parskip=0em
\item Communication in the log manager is extremely sparse. Most update transactions will issue just one global atomic compare-and-swap before committing, even in the presence of high contention and corner cases such as full log buffer or log file rotations.
\item Transactions maintain log records privately while in flight, and aggregate them into large blocks before inserting them into the log.
\item Transactions can acquire their commit LSN before entering pre-commit, thus simplifying the latter significantly (as all committing transactions agree on their relative commit order).
\item Large object writes can be diverted to secondary storage, requiring only an indirect pointer in the actual log.
\end{list}

The central feature of the log---{\em a single global atomic operation per insert}---rests on a key observation: while the LSN space must be monotonic, it need not be contiguous as long as sequence numbers can be translated efficiently to physical locations on disk. Therefore, each log sequence number (LSN) consists of two parts: the higher-order bits identify an offset in a logical LSN space, while the lowest-order bits identify the physical log segment file the offset maps to. There are a fixed number of log segments in existence at any time (16 in our prototype), but segments may be arbitrarily large and are sized independently of each other.\footnote{A system can thus scale log segment sizes upward to handle higher loads, and downward to conserve disk space.} Placing the segment number in low-order bits preserves the total of log offsets.

\labeledfigure{ermia-log}{ERMIA's log manager.}

\figref{ermia-log}(a) illustrates how the system converts between LSN and physical file offsets. Each modulo segment number is assigned a physical log segment, which consists of a start and end offset, and the name of the file that holds the segment's log records. The file name is chosen so the segment table can be reconstructed easily at start up/recovery time, even if the current system's log segment size is different than that of the existing segments. Given the mapping table, LSN can be validated and converted to file offsets using a constant-time look up into the table.

Because we allow holes in the LSN space, acquiring space in the log is a two-step process. First, the thread claims a section of the LSN space by incrementing a globally shared log offset by the size of the requested allocation. Then, having acquired this block of logical LSN space, the transaction must validates it against various corner cases:

\begin{list}{$\bullet$}{\usecounter{mycounter}\leftmargin=1em}
  \itemsep=0.1em
  \parsep=0em
  \parskip=0em
\item Log segment full. If the block straddles the end of the current segment file, it cannot be used and a skip record is written in its place to ``close'' the segment. The thread competes to open the next segment file (see below) before requesting a new LSN.
\item Between segments. Threads that acquire a log block that starts after the current segment file must compete to open the next segment. Blocks preceding the winner's block must be discarded as they do not correspond to a valid location on disk; losers retry.
\item Log buffer full. The transaction retains its LSN but must wait for the corresponding buffer space to drain before using it.
\end{list}

Once a thread validates its LSN offset, it combines the segment and offset to form a valid LSN. A transaction can request log space at any time, for example to enter pre-commit with a valid commit LSN or to spill an oversized write footprint to disk; should it later abort, it simply writes a skip record.

\figref{ermia-log}(b) illustrates the various types of log blocks that might exist near a log segment boundary. Skip records in the middle of the segment come from aborted transactions (or are overflow blocks disguised as skip records). The skip record at the end of the segment ``closes'' the segment and points to the first valid block of the next segment. The ``dead zone'' between segments $i$ and $i+1$ contains blocks that lost the race to open segment $i+1$; these do not map to any location on disk and should never be referenced. 

Decoupling LSN offsets from log buffer and log file management simplifies the log protocol to the point that a single atomic operation suffices in the common case. There are only two times a transaction might require additional atomic operations: (a) Occasionally, a segment file will close and several unlucky transactions will race to open the next one. This case is very rare, as segment files can be hundreds of GB in size if desired. (b) A large write transaction may not be able to fit all of its log records in a single block, and so would be forced to write some number of overflow log blocks in a backward-linked list before committing. We expect this case to {\em reduce} contention, because a thread servicing large transactions---even with overflow blocks---will not visit the log nearly as often as a thread servicing short requests.

Because the log allocation scheme is fully non-blocking, there is no reliable way to know whether a thread has left. A thread with a reference to a segment that is about to be recycled could end up accessing invalid data (e.g. from segment $i+16$ rather than segment $i$). Therefore, we use an epoch manager to ensure that stragglers do not suffer the ABA problem when the system recycles modulo segment numbers, and that background cleaning has migrated live records to secondary storage before a segment is reclaimed (reassigning its modulo segment number effectively removes it from the log's ``address space'').

As a final note, we observe that, for many objects in the system, their physical location is the log record that created their latest version. We therefore extend the LSN with a notion of ``address spaces'' that identify whether the offset points to the log, secondary storage, or memory; the indirection array stores these augmented LSN to very cleanly implement anti-caching \cite{DeBrabantPTSZ13}. 

\subsection{Epoch-based resource management}
\seclabel{design:epochs}

Epoch-based memory management--exemplified by RCU \cite{McKenneyS98}---allows a system to track readers efficiently, without requiring readers to enter a critical section at every access. Updates and resource reclamation occur in the background once the system can prove no reader can hold a reference to the resource in question. Readers inform the system when they become {\em active} (intend to access managed resources soon) and {\em quiescent} (no longer hold any references to managed resources). These announcements are decoupled from actual resource accesses in order to amortize their cost, as long as they are made reasonably often. Ideal points in a database engine would include transaction commit or a worker thread going idle. Resource reclamation then follows two phases: the system first makes the resource unreachable to new arrivals, but delays reclaiming it until all threads have quiesced at least once (thus guaranteeing that all thread-private references have died). Once a resource is proven safe to reclaim, reclamation proceeds very flexibly: worker threads are assigned to reclaim the resources they freed and a daemon thread performs cleanup in the background.

We have developed a lightweight epoch management system that can track multiple time lines of differing granularities in parallel. A multi-transaction-scale epoch manager implements garbage collection of dead versions and deleted records, a medium-scale epoch manager implements read-copy-update (RCU) that manages physical memory and data structure usages \cite{McKenneyS98}, and a very short timescale epoch manager tracks transaction IDs (TIDs), which we recycle aggressively (see details in \secref{tm}).

The widespread and fine-grained use of epoch managers is enabled by a very lightweight design we developed for ERMIA. It has three especially useful characteristic. First, the protocol for a thread to report activation and quiescent points is lock-free and threads interact with the epoch manager through thread-private state they grant it access to. Thus, activating and quiescing a thread is inexpensive. Second, threads can announce conditional quiescent points: if the current epoch is not trying to close, a read to a single shared variable suffices. This allows highly active threads to announce quiescent points frequently (allowing for tighter resource management) with minimal overhead in the common case where the announcement is uninteresting. Third, and most importantly, ERMIA tracks three epochs at once, rather than the usual two.

In a traditional epoch management scheme, the ``open'' epoch accepts new arrivals, while a ``closed'' epoch will end as soon as the last straggler leaves. Unfortunately, this arrangement cannot differentiate between a straggler (a thread that has not yet quiesced in a long time) and a busy thread (one which quiesces often but is likely to be active at any given moment). Thus, when an epoch closes, most busy threads in the system will be flagged as stragglers---in addition to any true stragglers---and will be forced to participate in an expensive straggler protocol designed to deal with non responsive threads. The ERMIA epoch manager, in contrast tracks a third epoch, situated between the other two, which we call ``closing.'' When a new epoch begins, all currently active threads become part of the ``closing'' epoch but are otherwise ignored. Only when a third epoch begins does the ``closing'' epoch transition to ``closed'' and check for stragglers. Active threads will have quiesced and migrated to the current (open) epoch, leaving only true stragglers---if any---to participate in the straggler protocol.

Although the three-phase approach tracks more epochs, the worst-case duration of any epoch remains the same: it cannot be reclaimed until the last straggler leaves. In the common case where stragglers are rare, epochs simply run twice as often (to reflect the fact that threads have two epoch-durations to quiesce). 

\subsection{Transaction management}
\seclabel{tm}
At its lowest level, the transaction manager is responsible to provide information about transactions that are currently running, or which recently ended, and this is partly achieved by allocating TIDs to all transactions and mapping those TIDs to the corresponding transaction's state. In our system, this is especially important because the CC scheme (see below) makes heavy use of TIDs to avoid corner cases.

Similar to the LSN allocation scheme the log manager uses, ERMIA's transaction manager assigns a TID to each transaction that combines an offset into a fixed-size table (where transaction state is held) with an epoch that identifies the transaction's ``generation'' (distinguishing it from other transactions that happened to use the same slot of the TID table).

Each entry in the TID table records the transaction's full TID (to identify the current owner), start timestamp (an LSN), end timestamp (if one exists), and current status. The latter two fields are the most heavily used by the CC scheme, as transactions stamp records they create with their TID and only change that stamp to a commit timestamp during post-commit. During the cleanup period, other transactions will encounter TID-stamped versions and must call into the transaction manager to learn the true commit status/stamp. Such inquiries about a transaction's state can have three possible outcomes: (a) the transaction could still be in flight, (b) the transaction has ended and the end stamp is returned, or (c) the supplied TID is invalid (from a previous generation). In the latter case, the caller should re-read the location that produced the TID---the transaction in question has finished post-commit and so the location is guaranteed to contain a proper commit stamp.

The TID table has limited capacity (currently 64k entries), and so the generation number will change frequently in a high-throughput environment. The TID manager thus tracks which TIDs are currently in use---allowing them to span any number of generations---and recycles a given slot only once it has been released. Because the system only handles a limited number of in-flight transactions at a time (far fewer than 64k), at most a small fraction of the TID table is occupied by slow transactions. 

In order to serve TID inquiries, and handle allocation/deallocation of TIDs across multiple generations (all of which are quite frequent), we use only lock-free protocols, with an epoch manager, running at the time scale of a typical TID allocation---to detect and deal with stragglers who are delayed while trying to allocate a TID; this is only possible because the epoch manager is so lightweight. Without an epoch manager, a mutex would be required to prevent bad cases such as double allocations and thread inquiries returning inaccurate results.

%in the system is assigned a slot in a global transaction state table when it begins. This fixed-size table holds the transaction's begin time (which is the log's end LSN at the time it started), status, and end time (if applicable). TIDs are a combination of table offset and epoch, with an epoch manager to prevent entries from being recycled too soon. Update transactions write their TIDs into each version they create, change their status to pre-commit, acquire a commit LSN (or are given one by an impatient peer), and finally commit atomically by changing their status to ``committed.'' A post-commit cleanup step involves replacing the transaction's TID with its commit LSN, at which point the state table entry is no longer needed and can be recycled by the epoch manager. Other transactions that encounter a TID in a version can reliably verify its commit status and age by visiting the transaction state table, and---if necessary---will help a peer enter pre-commit by acquiring a log block on its behalf. All of this is done entirely lock-free, without using mutex locks. 

\subsection{Indirection arrays}
\seclabel{design:oid}
The indirection arrays used in ERMIA are similar to the ones proposed in the literature \cite{SadoghiRCB13,Diaconu+13}. All logical objects are identified by an object ID (OID) that maps to a slot in an OID array that contains the physical pointer to data. The pointer may reference disk, or a chain of versions stored in memory. As with Hekaton, uncommitted versions are never written to disk; but unlike Hekaton, we dispense with delta records (too expensive to apply) and use pure copy-on-write. New versions can be installed by an atomic compare-and-swap operation, and an uncommitted record at the head of the chain constitutes a write lock for CC schemes that care to track write-write conflicts (as most do). 

\subsection{Concurrency control}
\seclabel{design:cc}

ERMIA has been designed from the ground up to allow efficient implementations of a variety of CC mechanisms, including Silo/Hekaton flavored read-set validation and snapshot isolation. Moreover, it allows efficient implementations of non-trivial CC schemes (other than simplistic read-set validation) to handle heterogeneous workloads gracefully and make them serializable (e.g., complex financial transactions represented by TPC-E~\cite{TPCE}). Different components in ERMIA work together to make this possible: indirection arrays allow cheap (almost free) multi-versioning; at an extremely low overhead, the log gives total commit ordering, which is the key to implement snapshot isolation; the transaction manager can also help determine a version's age easily, making resource management straightforward.

Depending on the target workload, ERMIA can use read set validation, two-phase commit, snapshot isolation, or even serializable snapshot isolation (SSI)~\cite{Cahill08RF}. We focus on handling \textit{read-mostly} workloads gracefully while maintaining comparable performance for short, update-intensive workloads. Although not serializable, snapshot isolation is an ideal choice to start from: long, read-mostly transactions will have much higher chance to survive when compared to read-set validation, and ERMIA can provide SI at virtually zero additional cost. When desired, ERMIA uses SSI to guarantee serializability. In particular, our variant of SSI takes advantage of ERMIA's design to eliminate the high overheads and communication bottlenecks that plague all existing SSI implementations the authors are aware of. The following paragraphs briefly highlight the SSI variant we developed for ERMIA.

\labeledfigure{ssi-dang-struct}{The ``dangerous structure'' that must exist in every serial dependency cycle under snapshot isolation. Here, T1 and T2 read versions that are  overwritten by T2 and T3, respectively, and T3 commits before both T1 and T2.}

{\bf Stamp-based tracking.}
% basically copied from Ryan's SSI simulator.
SSI ensures serializability by tracking the ``dangerous structure'' that must exist in every serial dependency cycle under snapshot isolation. As shown in~\figref{ssi-dang-struct}~\cite{Cahill08RF}, transaction T1 or T2 must abort if a read-write dependency exists and T3 committed first. Since ERMIA provides global ordering through the log, and commit timestamps are available during pre-commit, tracking and detecting the dangerous structure becomes straightforward and multiple transactions are allowed to make pre-commit progress simultaneously. In contrast, many SSI implementations allow only one transaction at a time to enter pre-commit, and fully decentralized OCC schemes cannot easily provide SI because the lack of a global commit order makes it expensive or even impossible to determine which of two transactions precedes the other.

The basic tracking for SSI is facilitated by maintaining three timestamps in each transaction: \texttt{s0}, \texttt{s1}, and \texttt{s2}.\footnote{These timestamps are copied to the versions that transaction creates during post-commit, allowing TID table entries to be reclaimed in a timely manner, but other strategies are possible as well.} \texttt{s0} is the transaction's own commit time. \texttt{s1} is the ``successor stamp,'' and records the smallest \texttt{s0} of any transaction that overwrote any version in T's read footprint. Similarly, \texttt{s2} records the smallest \texttt{s1} of any transaction that overwrote T's read footprint. Finally, each version maintains an \texttt{rstamp} that records the latest commit timestamp of any reader.

Whenever transaction T performs a read, it checks whether \texttt{s2} is set on that version. If so, T is T1 of a dangerous structure where T3 committed first; T2 already committed, so T1 must abort to prevent a potential cycle. T next checks whether \texttt{s1} is set. If so, T is a potential T2 of a cycle (the ``pivot'') and we prefer to abort it if T has overwritten any in-flight readers: doing so provides the ``safe restart'' property discussed earlier. If no such reader exists, T is allowed to continue, but records the smallest \texttt{s1} it encounters in order to check for late-arriving readers during pre-commit. T also remembers the smallest \texttt{s0} of any version it has read. At pre-commit, T checks again whether it is the ``pivot'' of any dangerous structure involving an in-flight T1, aborting if so. Otherwise, T can enter post-commit and finalize stamps in each version it created and overwritten. Versions created by T will have T's commit timestamp as its s0, while versions overwritten by T will have T's commit timestamp as its \texttt{s1}, and the remembered \texttt{s1} as its \texttt{s2}. Finally, versions read by T will have T's commit timestamps its \texttt{rstamp} if it is smaller than T's commit timestamp. There is no need to update in-flight readers because of the pivot test already performed. We next describe how the commit protocol works.

{\bf Commit protocol.}
We divide the commit process into two parts: pre-commit and post-commit. Pre-commit under SSI involves three major steps: (1) obtain a commit stamp; (2) examine the read set to learn the smallest \texttt{s1}, as shown in the first half of Algorithm~\ref{alg-ssi-commit}; (3) examine overwritten versions by iterating the write set to update T's rstamp. As shown in the second half of Algorithm~\ref{alg-ssi-commit}, during step 3, T must abort if it has as valid \texttt{s1} and it has overwritten any in-flight readers or any version with \texttt{rstamp} larger than \texttt{s1} (i.e., T will be the T2 in a dangerous structure if allowed to commit). Crucially, these pre-commit checks can be performed in parallel by multiple transactions, because all transactions in pre-commit have valid commit stamps and therefore agree on which of members of a potential dangerous structure attempted to commit first. If the transaction survived pre-commit, version stamps will be updated. Note that these version stamps (except the commit stamp) are only needed at run time; the versions stored in the log and on disk do not require SSI checks because no in-flight transactions at the time of a crash could have committed. The above commit protocol largely simplifies SSI checks using ERMIA's infrastructure.

In the rest of this section we describe major optimizations that further improve SSI's performance, followed by a description of the phantom protection mechanism we use.

\begin{algorithm}
\begin{algorithmic}[1]
\STATE {\bf Input:} Committing transaction \textit{T}
\STATE T.cstamp = log.next\_commit\_stamp()
\STATE /* Get the smallest s1 from all reads. */
\STATE \texttt{ts1} = 0;
\FOR{$each~read~version~\texttt{v}:$}
\STATE T$_s$ = overwriter transaction of \texttt{v}
\IF{$T_s~exists~and~T_s.cstamp~<~cstamp$}
\STATE Spin until T$_s$ is resolved
\IF{$T_s~is~committed$}
\STATE T.s1 = min(T$_s$.cstamp, T.s1)
\ENDIF
\ELSE
\STATE T.s1 = min(T$_s$.cstamp, v.s1)
\ENDIF
\ENDFOR
\STATE
\STATE /* Check writes. */
\FOR{$each~written~version~\texttt{w}:$}
\IF{$\texttt{w}~is~an~insert$}
\STATE \textbf{continue}
\ENDIF
\STATE \texttt{v} = the overwritten version
\FOR{$each~reader~transaction~T_r:$}
\IF{$T_r~is~active$}
\STATE T.abort()
\ENDIF
%\IF{$T_r.cstamp~<~T.cstamp$}
%\STATE continue
%\ENDIF
\STATE T.rstamp = max(T.rstamp, w.rstamp)
\IF{$T.rstamp\geq~T.s1$}
\STATE T.abort()
\ENDIF
\ENDFOR
\ENDFOR
\end{algorithmic}
\label{alg-ssi-commit}
\caption{ERMIA SSI commit protocol.}
\end{algorithm}

{\bf Lightweight reader tracking.}
As described in Algorithm~\ref{alg-ssi-commit}, the writer needs to track all readers of versions it overwrites, and verify readers' status at pre-commit. To achieve this, one might maintain a linked list of reader TIDs in each version, or in a system-wide lock manager of sorts (Postgresql SSI follows the latter approach). Readers add themselves to the list when they read the version and unlink when they commit or abort. However, maintaining such a list---whether in version headers or a lock manager---increases the effective size of a version significantly and causes significant extra cache misses, especially for small versions that are frequently read by multiple transactions (a common case for main-memory systems with massively parallel hardware). Since ERMIA executes each transaction from beginning to end using a single thread, without changing transaction context, we employ an optimization to embed reader information compactly in versions. The system maintains a small table with one entry per worker thread, with each entry assigned to one worker thread at start-up. The worker thread stores the TID of any transaction it is currently executing in this table. The reader list for a given version then shrinks to a bitmap, with a set bit indicating that the corresponding worker thread has read the version. An overwriting transaction can thus easily discover all readers of a version, by reading thread TIDs for all set bits during pre-commit.

{\bf Read optimization.}
Maintaining read sets is a major performance overhead not only for SSI, but also for OCC schemes that guarantee serializability. Long reader transactions are especially expensive to track, and existing optimizations usually handle only a limited class of read-only transactions~\cite{PortsG12}, while providing no benefit for \textit{read-mostly} transactions. To address this issue, we observe that most tuples in a database tend to be ``old'', i.e., not recently updated. Long, read-mostly transactions tend to access a large number of these old tuples. Based on this observation, we optimize reads by setting an age threshold such that any transaction reading an ``old'' version can commit without verifying that read. In detail, a version's age is determined by subtracting its creation timestamp from the accessing transaction's begin timestamp. A version is determined to be ``old'' if its age is larger than a predefined threshold, and will not be inserted to the read set. This greatly reduces tracking overhead, but the reader can no longer perform complete SSI checks during pre-commit. Instead, the responsibility for checking those versions passes to any transaction that overwrites an old tuple: the writer must assume that the old tuple does, in fact, have readers, and will abort if that assumption makes it the pivot in a dangerous structure. 

The above method can generate false positives, because the writer must conservatively assume a reader exists, when this may not be true. To reduce such false positives, we employ a \texttt{bstamp} field in each old version to record the largest reader's begin timestamp (which the reader maintains at the time of the read). The writer will only abort if at least one reader found in worker thread TID list has a begin timestamp older than the version's \texttt{bstamp}; this extra check eliminates false positives for records that have not been read recently.

{\bf Phantom protection.} Although SSI prevents serialization dependency cycles, serialization failures can also arise due to {\em phantoms}, or insertion of new records into a range previously read by an in-flight transaction. ERMIA is amenable to various phantom protection strategies, such as hierarchical and key-range locking~\cite{KimuraGK12,Lomet93}. Since ERMIA's current implementation
is a heavily modified version of Silo~\cite{TuZKLM13}, we inherit the latter's tree-version validation strategy. The validation is lightweight, if conservative, and using it also provides a more fair comparison in the evaluations that follow. The basic idea is to track and verify tree node versions, taking advantage of the fact that any insertion into an index will change the version number of affected leaf nodes. In addition to maintaining the read set, same as Silo, ERMIA also maintains a \textit{node set}, which maps from leaf nodes that fall into the range query to node versions. The node set is examined after pre-commit. If any node's version has changed, the transaction must abort to avoid a potential phantom. Interested readers may refer to~\cite{TuZKLM13} for more details.

\subsection{Recovery}
\seclabel{design:recovery}

Recovery in ERMIA is straightforward because the log contains only committed work; OID arrays are the only real source of complexity. Although OID arrays are normal objects, which are themselves reached through a master OID array, they can occupy hundreds of MB or more and are thus far too large for copy-on-write to be practical at each record insertion or deletion. Instead, the OID array objects are updated in place to avoid overloading the log, and are thus effectively volatile in-memory data structures. However, they are needed to find all other objects in the system (including themselves), so ERMIA employs a combination of fuzzy checkpointing and logical logging to maintain OID arrays properly across crashes and restarts. All OID arrays are periodically copied (non-atomically) and the disk address of each valid OID entry is dumped to secondary storage after recording a checkpoint-begin record in the log. A checkpoint-end record records the location of a the fuzzy snapshot once the latter is durable. The location of the most recent checkpoint record is also recorded in the name of an empty checkpoint marker file in the file system. During recovery, the system decodes the checkpoint marker, restores the OID snapshots from the checkpoint, then rolls them forward by scanning the log after the checkpoint and replaying the allocator operations implied by insert and delete records. The replay process examines only log block headers (typically under 10\% a block) and does {\em not} replay the insertions or deletions themselves (which are safely stored in the log already). A similar process could potentially be applied to other internal data structures in the system, such as indexes, but we leave that exploration to future work. It is important to note that the process of restoring OID arrays is exactly the same when coming up from either a clean shutdown or a crash---the only difference is that a clean shutdown might have a more recent checkpoint available.

In summary, because the log is the database, recovery only needs to rebuild the OID arrays in memory using sequential I/O; anti-caching will take care of loading the actual data, though background pre-loading is highly recommended to minimize cold start effects.

%\subsection{Prototype Implementation}
%\seclabel{design:prototype}
%
%We implement a prototype of the ERMIA architecture and measure its performance.  For the implementation of the prototype we use a large fraction of the publicly available Silo codebase~\footnote{Silo's codebase can be downloaded from: https://github.com/stephentu/silo.}.  Silo uses the Masstree \cite{MaoKM12} as a cache-efficient index structure. 
