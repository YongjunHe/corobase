\documentclass[preprint]{sig-alternate-nocprt}
\usepackage[nocapsections,obey,smallbib]{fixacm}

%\usepackage{color}
\usepackage{graphicx}
%\usepackage{balance}
%\usepackage{subcaption}
%\usepackage{setspace}
\usepackage[hyphens]{url}

%\usepackage[bookmarks=false,hidelinks,pdftitle={Design Tradeoffs for Fast, Robust Database Systems}]{hyperref}

\pagestyle{empty}

\begin{document}
%\captionsetup[figure]{labelfont=bf,textfont=bf}
%\captionsetup[subfigure]{labelfont=normalfont,textfont=normalfont}

\title{Robust Concurrency Control for Heterogeneous Workloads on Modern Hardware}

\numberofauthors{2}
\author{
\alignauthor
%Tianzheng Wang\\
%       \affaddr{Department of Computer Science}\\
%       \affaddr{University of Toronto}\\
%       \email{{\large \sf tzwang@cs.toronto.edu}}
%\alignauthor
%Ryan Johnson\\
%       \affaddr{Department of Computer Science}\\
%       \affaddr{University of Toronto}\\
%       \email{{\large \sf ryan.johnson@cs.utoronto.ca}}
}

\maketitle

%1. new hw trend has led to new systems
The trend of large main memories and massively parallel processors has led to many new OLTP systems recently~\cite{HStore,Hyper,Hekaton,Silo} with goal of extracting the most performance benefits from new hardware. These systems utilize spacious main memory to fit the whole working set in DRAM with simplified, memory-friendly data structures, and takes advantage of multicore and multi-socket hardware to allow a much higher level of parallelism compared to conventional database systems.

%2. how current workloads make cc important again.
Meanwhile, database workloads are also evolving to become increasingly heterogeneous, blending the gap between transaction and analytical processing. Workloads nowadays are often mixed with reads and writes, showing a higher read to write ratio. Some of them become longer because of the analytical portion. The latest TPC-E~\cite{TPC-E} benchmark also reflected this trend, with a read to write ratio of 9.7:1, while the number for TPC-C is 1.9:1~\cite{TPC-Compare}. With modern parallel hardware, many more concurrent transactions are allowed to co-exist, possibly with overlapped footprints. The need to handle such heterogeneous workloads on modern hardware makes concurrency control (CC) an important area of research again.

%3. current schemes: 2PL blocks + deadlock issues. existing schemes in new systems (mostly OCC) suffer (1) long tx with other tx going on can't commit (2) write clobber read
Two representative CC mechanisms in existing systems are two-phase locking (2PL) and optimistic concurrency control (OCC). 2PL is usually found in most traditional disk-oriented systems, and OCC is adopted by more recent systems~\cite{Hekaton,Silo}. Though being correct and serializable, 2PL has always been criticized for having to block transactions and dealing with deadlocks. Recent systems have different architectures and approaches to CC. But most of them are optimistically single-versioned schemes, sharing a common weakness of having excessive false positives because \textit{any version overwritten before the reader commits will lead to an abort}. When handling heterogeneous workloads, long transactions tend to be aborted if other short, concurrent transactions overwrite any of its read set; short transactions with both reads and writes tend to cause each other to abort because of overlapped read/write sets. Ideally, the CC scheme should be \textit{robust} enough to avoid such weaknesses and produce reasonably well commit/abort ratios even in face of unbalanced heterogeneous workloads.

%4. attempts on new schemes such as SI/SSI, and 2PL+ wait depth limit
There have been some attempts to solve this problem with variants of snapshot isolation (SI) and 2PL. For example, serializable snapshot isolation~\cite{SSI} enhances the vanilla SI with seralizability guarantees and has been implemented in PostgreSQL. However, more efforts are still needed to make it scalable on modern parallel hardware~\cite{ScalableSSI}. 2PL with wait depth limited also shows promising results compared to strict 2PL. Another different approach is to maintain dependent transactions as a directed acyclic graph and only push delta results when transaction inputs change~\cite{LogicBlox}. Improvements on main stream CC schemes, such as multi-version concurrency control and basic time-stamp ordering and its variants such as partition-level locking also show promising results for certain workloads, it has been shown that none of the main stream CC schemes can scale well at a core count as high as one thousand~\cite{CCAbyss}.

%5. also arthictecture affects what CC a system can have. we need a system that enables good cc.
System architecture and design decisions also affect the design of CC schemes. For example, systems that uses OCC might prefer to have fewer communication points and operate in a decentralized fashion, thus also avoiding a total order. This in turn limits the chance of using CC schemes such as SI and SSI for handling heterogeneous workloads.

%6. a little on our system.
One approach to solving this dilemma is reconsidering design decisions on system architecture and the CC scheme. One might consider bringing the total order back in order to have better CC schemes such as SSI to handle heterogeneous workloads on modern hardware, but will then have to make sure the centralized communication points do not become the new bottleneck. Aspects such as logging, recovery, and the robustness of the CC scheme including the decision to blocking threads, must be considered together as part of the whole system design.

\bibliographystyle{abbrv}
\bibliography{ref}
\end{document}
