%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Evaluation}
\seclabel{eval}
In this section, we compare the performance of ERMIA and Silo, a representative of the lightweight OCC camp, from two perspectives: (1) the impacts of concurrency control scheme on performance and (2) scalability of the underlying storage manager. The purposes of our evaluation is two-fold. First, we show that although recent OCC proposals for main-memory OLTP perform extremely well, they are only suitable for a small fraction of transactions---short update transactions---primarily due to limitations imposed by their (tightly-integrated) concurrency control mechanism. Second, we demonstrate that ERMIA is able to offer robust concurrency control, maintaining high performance even under contentious workloads without sacrificing features. At the same time, ERMIA is also able to match the performance of specialized OCC solutions when running their favorite workloads. We also show that because of carefully orchestrated communications and interactions in both the physical and logical levels, ERMIA scales well under various types of workloads.

\subsection{Experimental setup}
\seclabel{eval:setup}
% System
We used a 4-socket server with 6-core Intel Xeon E7-4807 processors, for a total of 24 physical cores and 48 hyperthreads. The machine has 64GB of RAM. All worker threads were pinned to a dedicated core and NUMA node to minimize context switch penalty and inter-socket communication costs. Log records are written to tmpfs asynchronously. We measure the performance three systems: Silo, ERMIA with SI (ERMIA-SI), and ERMIA with SSI (ERMIA-SSI). Next we introduce the benchmarks.

\subsection{Benchmarks}
We run the TPC-C and TPC-E benchmarks on all variants. In each run, we load data from scratch and run the benchmark for 40 seconds.
\tianzheng{prob. bullet points.}

\textbf{TPC-C.}

\textbf{Standard TPC-E.}
TPC-E is a new standard OLTP benchmark. Although TPC-C has been dominantly used to evaluate OLTP systems performance for the past few decades, TPC-E was designed as a more realistic OLTP benchmark with modern features. \tianzheng{cite HANA paper} TPC-E models brokerage firm activities and has more sophisticated schema model and transaction execution control. It is also known for higher read to write ratio than TPC-C.


%TPCE + contention
\textbf{Analytic-heavy TPC-E.}
Since TPC-E is not a heterogeneous workload, we introduce a new read-mostly analytic transaction that evaluates total assets of a random customer account group and inserts the analytic activity log into \textit{analytic\_history} table. Total assets of the accounts are computed by joining \textit{holding\_summary} and \textit{last\_trade} tables. The vast majority of contentions occur between the analytic transaction and \textit{trade-result} transaction.
The parameters we set for this experiment are 5000 customers, 500 scale factor and 10 initial trading days(the initial trading day parameter was limited by our machine's memory capacity).
The analytic transaction takes 20\% out of the original TPC-E transaction mix. % TODO. new workload mix, footnote?

\subsection{TPC-E}
\seclabel{eval:tpce}
We first explore how ERMIA and Silo react to contention in the TPC-E.

\labeledfigurewide{fig-tpce-robustness}{Total throughput (left); analytic transaction throughput(right) of TPCE-hybrid, varying contention degree}

% chart - Commit rates / Abort rates, varying contention degree. 
\figref{fig-tpce-robustness}(left) shows the throughput with 24 worker threads, varying contention degree when we run TPC-E with the analytic transaction(TPCE-hybrid). To vary contention degree, we adjust the size of a customer account group to be scanned by the analytic transaction from 5\% to 20\%. The key observation is that SILO is more sensitive to contention. ERMIA-SI outperforms SILO at all contention degree and the performance gap gets larger as we increase contentions. In case of ERMIA-SSI, it falls behind the SILO by 26\% at 5\%, however, it starts to close the gap at higher contention level and finally catches up with SILO at 20\%. 
The main cause for the result is that the contention in the workload imposed heavy pressure on the SILO's OCC protocol; OCC enforced transactions to abort even if single tuple of their read-set is invalidated by updaters. Meanwhile, ERMIA endured the contention by protecting the read-sets from updaters effectively with snapshot isolation and distributed the contentions across multiple versions. 
%ERMIA-SI produces commits more than ten times than SILO at 20\% scan range. %TODO. put SSI number also.
we now focus on the throughput of the analytic transactions to see how well both systems support analytics on transactional data set. As shown in the \figref{fig-tpce-robustness}(right), the analytic transactions were shoot down massively even under small contention degree on SILO andthe analytic transaction throughput drops more sharply than ERMIA, as we double transaction footprint. We can derive that the massive reader aborts critically affected the overall throughput. This is obviously a negative implication to run heterogenous workload under OCC. ERMIA provides balanced query/transaction performance, while SILO extremely gives penalty to read-intensive long transactions. 

% scalability 
\labeledfigurewide{fig-tpce-scalability}{Throughput when running TPCE-hybrid(left); TPCE-original(right)}
\labeledfigurewide{fig-tpcc-scalability}{Throughput when running TPCC-contention(left); TPCC-partitioned(right)}
In \figref{fig-tpce-scalability}(left), we fix contention degree to 20\% scan range and see scalability trends, increasing the number of workers. Overwhelmed by contentions, SILO does not achieve linear scalability, even with its excellent scalability of the underlying physical layer. ERMIA benefits from its robust CC scheme and its storage manager did not forestall achieving linear scalability. This figure shows that not only the scalability of underlying physical layer, but also CC scheme's capability to deal with contention in logical level dictates overall performance. We also performed the same experiment in the original TPC-E, without the analytic transaction. As illustrated in \figref{fig-tpce-scalability}(right), both ERMIA-SI and SILO achieve linear scalability over 24 cores. SILO does not suffer from contention, since TPCE-original has insignificant contentions. ERMIA-SSI delivered 83\% of ERMIA-SI performance with 24 threads due to the serializability cost.

\subsection{TPC-C} 
\seclabel{eval:tpcc}
We also run TPC-C where lightweight OCC camp shine. This experiment will be focusing on evaluating storage manager's scalability naturally, as TPC-C imposes little pressure on CC scheme. TPC-C is well-known for update-heavy workload and small transaction footprints. Also, it is well-partitionable workload; conflicts can be reduced even further by partitioning and single-threadinging on the partition, setting the almost ideal environment for the lightweight OCC. The only source of contention under partitioned TPC-C is multi-partition transaction, however, its impact on CC is dampened by small transaction footprints, avoiding the majority of read-write conflicts. 

% TPCC-original
We measure throughput of both systems in such partitioned TPC-C(TPCC-partitioned), varying the number of worker threads. All worker threads are given home warehouse and does not change the home warehouse during runtime. Cross partition transaction takes ??\%.\kk{TODO. fill out this number} \figref{fig-tpcc-scalability}(right) shows that both systems scale reasonably over 24 cores. Compared to SILO, ERMIA falls behind SILO by 10\% and 20\%, with SI and SSI respectively, at the peak performance for the following reasons. First, CC did not suppress throughput due to lack of contention. Second, SILO benefit from TPC-C's small cache footprint. Unlike SILO, ERMIA maintains multi-versions in the indirection array. It pays additional cache miss costs during traversing indirection array to find a visible version. This was invisible in the TPC-E because its large cache footprint supressed caching effect. Thus, SILO outperforms ERMIA in workloads where 1) contention is rare and 2) transaction footprint is small enough to take advantage of cache-optimization; TPC-C is the compelling example of such workload. ERMIA-SSI had extra 10\% cost to guarantee serializability due to serial dependency checking.

% TPCC-random WH
We now enforce worker threads to pick a random working partition, following non-uniform distribution, before starting transactions. The purpose of this modification is to bring reasonable amount of contentions in TPC-C and to see how the previous results change. As shown in the \figref{fig-tpcc-scalability}(right), we can see that SILO is more sensitive to contentions than ERMIA-SI. Compared to TPCC-partitioned, the throughput of SILO decreased by approximately 30\%, while ERMIA delivered 15\% lower performance, catching up with SILO.
%
%\subsection{Performance study}
%\seclabel{eval:perf-study}
%We performed factor analysis to figure out how much cost ERMIA pays for its critical components quantitatively. \figref{fig-tpcc-cycle} illustrates normalized CPU cycle breakdown in the TPCC-partitioned with 24 worker threads. As shown in the figure, the indirection array imposed 10\% extra cost which mainly came from last level cache misses; 32\% of cache misses occured in indirection array. In ERMIA-SSI, serializability check brought additional 10\% overhead.\kk{not sure about this number} In all systems, Masstree is the biggest bottleneck with more than 30\% out of total cycles. The overhead of log manager and total ordering were negligible(~4\%). This supports that fully-uncoordinated logging and abandonment of total order are not necessarily required to achieve scalability. Epoch-based resource managers also performed well without huge overhead(?\%)\kk{TODO. fill out overhead}.
