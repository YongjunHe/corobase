%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{ERMIA: Fast and robust memory-optimized OLTP}
\seclabel{design}
In this section, we start with an overview of ERMIA, and then describe the key pieces of ERMIA, with a focus on why we choose the design trade-offs we do.

\subsection{Overview}
ERMIA is designed around epoch-based resource management and an extreme efficient, centralized log. \figref{ermia-arch} shows the major components in ERMIA and how they interact with each other. The log manager, TID manager, and garbage collector are three major entities that uses the epoch-based resource management machinery. To avoid log buffer contention~\cite{WangJ14}, each transaction acquires its log private log buffer (hence its global start stamp) from the log manager. Upon commit, the transaction will fix its global order in the log via a single CAS instruction and flush all its log records in one go (see~\ref{subsec:logging} for details). Although both log buffers and TIDs are managed by the epoch-based resource management mechanism, they use separate epoch lengths and can track stragglers efficiently. Transaction threads access the database through indexes. Different from traditional tree structures which give access to data in the leaf level, in ERMIA we embed in each index an indirection array, which can provide easy implementation of snapshot isolation and garbage collection. Indexed by object IDs, each array entry points to a chain of historic version for the object (tuple). The garbage collector periodically goes over all version chains and remove unnecessary, superseded versions that are not needed by any transactions.
\labeledfigure{ermia-arch}{Architecture of ERMIA.}

\subsection{Logging}
\label{subsec:logging}
The log manager is a pivotal component in most database engines. It provides---if anything does---a centralized point of coordination that other pieces of the system build off of and depend on. ERMIA's log manager generates a commit log sequence number (LSN) for the transaction and reserves log buffer space for the transaction's log records with a \textit{single global atomic operation}. Achieving this required two key insights: first, the transaction can combine its log records into large blocks, avoiding the redundancy of writing individual log record headers and reducing the number of trips to the log. Second, the LSN space need not be contiguous as long as we can still convert easily between an LSN and the corresponding disk address.

The first property arises from our use of append-only storage. We achieve the second property by assigning each LSN to a ``segment'' and storing its segment number in the low order bits; the LSN's position on disk can be determined by looking up, and subtracting off, its segment's starting offset (read-only). Transactions race to ``open'' a new segment if they obtain an LSN past the end of the current one. Unlucky transactions holding an LSN in the gap between two segments can simply discard it and request a new one. Segments can be as large as 100GB or more, however, so overflows will be rare. Once an LSN and segment have been assigned, the transaction verifies availability of space in the log's circular memory buffer (again, read-only); only in case the buffer is full will transactions have to block pending space, but disk arrays can readily absorb the sequential write-only I/O stream.

An additional feature of the log is that transactions acquire a commit LSN before entering pre-commit, allowing validation of multiple transactions to proceed smoothly in parallel; depending on the outcome of pre-commit, a transaction either writes its log entries or a skip record (to abort) to the reserved log block.

Finally, because the shared counter is implemented as a wait-free linked list, the transaction can notify the log writer that its block is ready to be flushed by simply flagging its node as ``dead'' (a blind store). The log writer periodically scans the list and writes out all log blocks that precede the oldest ``live'' buffer allocation; transactions do not touch each others' nodes, and the log writer only reads them and flags dead nodes for the garbage collector.

\subsection{Epoch-based resource management}
\kk{why RCU? what is it good for? - classifies data into different time windows, cleanup is done by background without interreing with foreground operations. a building piece to achieve lock-free resource mgmt. } We have developed a lightweight epoch management system that can track multiple timelines of differing granularities in parallel. A multi-transaction-scale epoch manager implements garbage collection of dead versions and deleted records, a medium-scale epoch manager implements read-copy-update (RCU) that manages physical memory and data structure usages~\cite{McKenneyS98}, and a very short timescale epoch manager tracks transaction IDs (TIDs), which we recycle aggressively (see details in \secref{tm}).

The key to efficiency here is to avoid flagging stragglers unless it is absolutely necessary (because coordinating with non-responsive threads is very expensive). Therefore, ERMIA does not attempt to reclaim resources for epoch $N$ until epoch $N+2$ begins. This way, potential stragglers have all of epoch $N+1$ to quiesce without penalty; however, epoch $N+3$ cannot begin until the last straggler from epoch $N$ completes. This four-phase scheme communicates far less with stragglers than the traditional two-phase \tianzheng{three-phase?} approach while maintaining the same worst-case timing bound. It allows us to track epochs at a very fine granularity when necessary. 

\subsection{Transaction management}
\seclabel{tm}
Each transactions in the system is assigned a slot in a global transaction state table when it begins. This fixed-size table holds the transaction's begin time (which is the log's end LSN at the time it started), status, and end time (if applicable). TIDs are a combination of table offset and epoch, with an epoch manager to prevent entries from being recycled too soon. Update transactions write their TIDs into each version they create, change their status to pre-commit, acquire a commit LSN (or are given one by an impatient peer), and finally commit atomically by changing their status to ``committed.'' A post-commit cleanup step involves replacing the transaction's TID with its commit LSN, at which point the state table entry is no longer needed and can be recycled by the epoch manager. Other transactions that encounter a TID in a version can reliably verify its commit status and age by visiting the transaction state table, and---if necessary---will help a peer enter pre-commit by acquiring a log block on its behalf. 

\subsection{Indirection arrays}
\seclabel{design:oid}
\kk{citation to mo sadoghi and hekaton?}
The indirection arrays used in ERMIA are very similar to the ones proposed in the literature. All logical objects are identified by an object ID (OID) that maps to a slot in an OID array that contains the physical pointer to data. The pointer may reference disk, or a chain of versions stored in memory. As with Hekaton, uncommitted versions are never written to disk; but unlike Hekaton, we dispense with delta records (too expensive to apply) and use pure copy-on-write. New versions can be installed by an atomic compare-and-swap operation, and an uncommitted record at the head of the chain constitutes a write lock for CC schemes that care to track write-write conflicts (as most do). 

\subsection{Concurrency control}
\seclabel{design:cc}

ERMIA has been designed from the ground up to allow efficient implementations of a variety of CC mechanisms, including Silo/Hekaton flavored read-set validation and snapshot isolation. Moreover, it allows efficient implementations of non-trivial CC schemes (other than simplistic read-set validation) to handle heterogeneous workloads gracefully and make them serializable (e.g., complex financial transactions represented by TPC-E). The components in ERMIA work together to make this possible: indirection arrays allow cheap (almost free) multi-versioning; at an extremely low overhead, the log gives total commit ordering, which is the key to implement snapshot isolation; the transaction manager can help determine a version's age easily.

Depending on the targeted workload, ERMIA can use read set validation, two-phase commit, snapshot isolation, or even serializable snapshot isolation (SSI)~\cite{Cahill08RF}. We focus on handling \textit{read-mostly} workloads gracefully while maintaining comparable performance for short, update-intensive workloads. Although not serializable, snapshot isolation is an ideal choice to start from: long, read-mostly transactions will have much higher chance to survive when compared to read-set validation. ERMIA uses SSI to guarantee serializability. In particular, our variant of SSI takes advantage of ERMIA's design to avoid sacrificing much performance as virtually all existing SSI based systems do. Below we briefly highlight our SSI variant for ERMIA.

\labeledfigure{ssi-dang-struct}{The ``dangerous structure'' that must exist in every serial dependency cycle under SI. A non-serializable schedule will have T1 and T2 read versions that are later overwritten by T2 and T3, respectively, with a T3 which overwrites a version that was read by T2 commits first.}

{\bf Stamp-based tracking.}
% basically copied from Ryan's SSI simulator.
SSI ensures serailizability by tracking the ``dangerous structure'' that must exist in every serial dependency cycle under SI as shown in \figref{ssi-dang-struct}~\cite{Cahill08RF}: T1 or T2 must abort if the read-write dependency exists and T3 committed first. Since ERMIA provides global ordering through the log, tracking and detecting the dangerous structure becomes straightforward. We associate each version with three stamps: \texttt{s0}, \texttt{s1}, and \texttt{s2}. \texttt{s0} indicates the versions creation timestamp, which is the commit timestamp of the transaction that created this version. \texttt{s2} records the smallest successor stamp (\texttt{s1}) among all the transactions' reads. An \texttt{rstamp} is also maintained in each version to indicate the commit stamp of the latest reader transaction. Whenever transaction T performs a read, it checks whether \texttt{s2} is set on any version. If so, T must abort (being the T1 of a dangerous structure where T3 committed first). T next checks whether \texttt{s1} is set. If so, T is the ``pivot'' and we prefer to abort it if T has overwritten any in-flight readers. If no such reader exists, then T is allowed to continue but it must remember the smallest \texttt{s1} it encounters, in order to re-check overwritten readers during pre-commit. T also remembers the smallest \texttt{s0} of any version it has read. At pre-commit, T checks again whether it is the ``pivot'' of any dangerous structure involving an in-flight T1, aborting if so. Otherwise, T can enter post-commit and finalize the three stamps in each version it created, with \texttt{s0} being its commit stamp, \texttt{s1} as the remembered \texttt{s0}, and \texttt{s2} as the remembered \texttt{s1}, if any was seen. There is no need to update in-flight readers because of the pivot test already performed. We next describe how the commit protocol works.

{\bf Commit protocol.}
We divide the commit process into two parts: pre-commit and post-commit. Pre-commit under SSI involves three major steps: (1) obtain a commit stamp; (2) examine the read set to find out the smallest \texttt{s1}, as shown in the first half of Algorithm~\ref{alg-ssi-commit}; (3) examine overwritten versions by iterating the write set to update T's rstamp. As shown in the second half of Algorithm~\ref{alg-ssi-commit}, during step 3, T must abort if we found there is any in-flight readers of any overwritten version or T's rstamp is greater than the minimal s1 we obtained in step 2 (i.e., T will be the T2 in a dangerous structure if committed). If the transaction survived pre-commit, version stamps will be updated. Note that these version stamps (except the commit stamp) are only needed at runtime; the log will store the version data and header without SSI stamps.

\begin{algorithm}
\begin{algorithmic}[1]
\STATE {\bf Input:} Committing transaction \textit{T}
\STATE T.cstamp = get\_commit\_stamp()
\STATE /* Get the smallest s1 from all reads. */
\STATE \texttt{ts1} = 0;
\FOR{$each~read~version~\texttt{v}:$}
\STATE T = overwriter transaction of \texttt{v}
\IF{$T_s~exists~and~T_s.cstamp~<~cstamp$}
\STATE Spin until T$_s$ is resolved
\IF{$T_s~is~committed$}
\STATE T.s1 = min(T$_s$.cstamp, T.s1)
\ENDIF
\ELSE
\STATE T.s1 = min(T$_s$.cstamp, v.s1)
\ENDIF
\ENDFOR
\STATE
\STATE /* Check writes. */
\FOR{$each~written~version~\texttt{w}:$}
\IF{$\texttt{w}~is~an~insert$}
\STATE \textbf{continue}
\ENDIF
\STATE \texttt{v} = the overwritten version
\FOR{$each~reader~transaction~T_r:$}
\IF{$T_r~is~active$}
\STATE T.abort()
\ENDIF
\IF{$T_r.cstamp~<~T.cstamp$}
\STATE continue
\ENDIF
\STATE T.rstamp = max(T.rstamp, w.rstamp)
\IF{$T.rstamp\geq~T.s1$}
\STATE T.abort()
\ENDIF
\ENDFOR
\ENDFOR
\end{algorithmic}
\label{alg-ssi-commit}
\caption{ERMIA SSI commit protocol.}
\end{algorithm}

{\bf Lightweight readers tracking.}
To track reads of a certain version when the overwriter's pre-commits, one might maintain a list of the TIDs of readers in each version. Readers will register themselves to the list when they read the version and deregister when they commit or abort. However, embedding a list in version headers will increase the size of a version significantly, causing more cache misses, especially for versions that are frequently read by multiple transactions (which is common in main-memory systems with massively parallel hardware). Since ERMIA executes each transaction from beginning to end by a single thread, without changing transaction context, we use a centralized TID list to record all in-flight transactions, and a bitmap in each old version to indicate the positions of readers in the centralized list. A transaction will register itself by setting the bitmap and putting its TID in the list's slot that corresponds to its thread ID. Note that the TID list is indexed by thread ID, so there is no contention upon transaction registration. The writer will be able to find out all readers through the bitmap and TID list at pre-commit.

{\bf Read optimization.}
Maintaining read sets is a major performance overhead of SSI and most CC schemes that guarantee serializability, especially for transactions with long reads. Existing optimizations usually focus on avoiding certain read-only transactions to participate in SSI~\cite{PortsG12}, which will not help reduce the overhead for \textit{read-mostly} transactions. In particular, most tuples in a database should be ``old'', i.e., not frequently updated. Long, read-mostly transactions will therefore have a higher chance of accessing these old tuples. Based on this observation, we optimize reads by guaranteeing any transaction that has read ``old'' versions can commit, unless it violates other conditions that necessitates its abort (e.g., a write-write conflict). In detail, a version's age is determined by subtracting its creation timestamp from the accessing transaction's begin timestamp. A version is old if its age is larger than a predefined threshold. Old versions will not be inserted to the read set. As a result, there is no way for the reader itself to go through SSI checks as usual during pre-commit. At pre-commit, if a transaction (the ``writer'') found itself overwrote an old version that was read by another transaction, it will have to abort, so that the readers can safely commit, avoiding forming the dangerous structure.

The above method could generate false positives: the TID slot indicated by the set bit in an old version's bitmap might already be re-allocated to another transaction when the writer enters pre-commit. For example, the reader could have already committed by the time the writer enters pre-commit. Such a schedule does not indicate a cycle in the dependency graph (so far), but the writer still has to abort. To reduce such false positives, we employ a \texttt{bstamp} field in each old version to record the latest reader's begin time stamp. The writer will only abort if the reader found in the centralized TID list has a begin timestamp older than the version's \texttt{bstamp}, eliminating most false positives caused by TID slot reuse.

{\bf Phantom protection.} ERMIA is amenable to various phantom protection mechanisms, such as hierarchical and key-range locking~\cite{KimuraGK12,Lomet93}. Since ERMIA's our current implementation is based on Silo~\cite{TuZKLM13}, we opt for the same tree-version validation mechanism---a lightweight, conservative approach---for fair comparison in the evaluation. The basic idea is to track and verify tree node versions, taking advantage of the fact that insertion in Masstree will version number change in affected leaf nodes. In addition to maintaining the read set, same as Silo, ERMIA also maintains a \textit{node set}, which maps from leaf nodes that fall into the range query to node versions. The node set is examined after read/write set validation. If any node's version does not match with the latest, the transaction will abort. Under SI, the verification can be done right before post-commit, while for SSI we verify the node set as the last step of pre-commit. Interested readers may refer to~\cite{TuZKLM13} for more details.

\subsection{Recovery}
\seclabel{design:recovery}

Recovery in ERMIA is straightforward because the log contains only committed work. OID arrays are the only real source of complexity, as they are volatile in-memory data structures that make it possible to find all other objects in the system. Logical objects (records) are physically logged, while physical data (allocator state and OID array contents) use logical logging. OID arrays are themselves objects stored in a master OID array, but they are updated in place to avoid overloading the log, with changes replayed by a log analysis step that reads only log block headers. This analysis step is very fast, because the skipped-over log payloads account for 90\% or more of the total log. In order to support efficient recovery, system transactions occasionally checkpoint the OID arrays using a fuzzy checkpointing mechanism to minimize the impact on user transactions. Because the log is the database, recovery only needs to rebuild the OID arrays in memory; anti-caching will take care of loading the actual data, though background pre-loading is highly recommended to minimize cold start effects.

%\subsection{Prototype Implementation}
%\seclabel{design:prototype}
%
%We implement a prototype of the ERMIA architecture and measure its performance.  For the implementation of the prototype we use a large fraction of the publicly available Silo codebase~\footnote{Silo's codebase can be downloaded from: https://github.com/stephentu/silo.}.  Silo uses the Masstree \cite{MaoKM12} as a cache-efficient index structure. 
