%% -*- tex-main-file:"rcu-cc.tex" -*-

\section{Evaluation}
\seclabel{eval}
In this section, we compare the performance of ERMIA with SILO, a representative of the lightweight OCC camp, from two perspectives: 1)CC impact on performance and 2)scalability of underlying storage manager. 
The purposes of the performance evaluation are the followings. First, we want to show that the recent proposals for main-memory OLTP solutions with lightweight optimistic concurrency control schemes can perform well and are suitable for only on a limited subset of transactional applications, primarily due to limitations imposed by their (tightly-integrated) concurrency control mechanism. Second, the proposed ERMIA design offers a more efficient concurrency control, which allows its performance to remain high under contentious workloads. At the same time, ERMIA does not suffer significant sacrifices in performance, even in workloads where the specialized solutions shine. Lastly, we measure performance overhead of two CC schemes, SI and SSI, quantitatively. In addition to the CC impact on performance, we also show that ERMIA achieves multicore scalability thanks to scalable storage manager, mainly achieved by log manager, epoch resource manager and latch-free indirection array. 


\subsection{Experimental setup}
\seclabel{eval:setup}
% System
We used a 4-socket server with 6-core Intel Xeon E7-4807 processors, for a total of 24 physical cores and 48 hyperthreads. The machine has 64GB of RAM. All worker threads were pinned to a dedicated core and NUMA node to minimize context switch penalty and inter-socket communication costs. Log records are written to tmpfs asynchronously.

\subsection{TPC-E}
\seclabel{eval:tpce}
We first explore how ERMIA and SILO react to contention in the TPC-E which is a new standard OLTP benchmark. Although TPC-C has been dominantly used to evaluate OLTP systems performance for the past few decades, TPC-E was designed as a more realistic OLTP benchmark with modern features. TPC-E models brokerage firm activities and has more sophisticated schema model and transaction execution control. It is also known for higher read to write ratio than TPC-C. 

%TPCE + contention
Since TPC-E is not a heterogeneous workload, we introduce a new read-mostly analytic transaction that evaluates total assets of a random customer account group and inserts the analytic activity log into \textit{analytic\_history} table. Total assets of the accounts are computed by joining \textit{holding\_summary} and \textit{last\_trade} tables. The vast majority of contentions occur between the analytic transaction and \textit{trade-result} transaction.
The parameters we set for this experiment are 5000 customers, 500 scale factor and 10 initial trading days(the initial trading day parameter was limited by our machine's memory capacity).
The analytic transaction takes 20\% out of the original TPC-E transaction mix. % TODO. new workload mix, footnote?

\labeledfigurewide{fig-tpce-robustness}{Total throughput (left); analytic transaction throughput(right) of TPCE-hybrid, varying contention degree}

% chart - Commit rates / Abort rates, varying contention degree. 
\figref{fig-tpce-robustness}(left) shows the throughput with 24 worker threads, varying contention degree when we run TPC-E with the analytic transaction(TPCE-hybrid). To vary contention degree, we adjust the size of a customer account group to be scanned by the analytic transaction from 5\% to 20\%. The key observation is that SILO is more sensitive to contention. ERMIA-SI outperforms SILO at all contention degree and the performance gap gets larger as we increase contentions. In case of ERMIA-SSI, it falls behind the SILO by 26\% at 5\%, however, it starts to close the gap at higher contention level and finally catches up with SILO at 20\%. 
The main cause for the result is that the contention in the workload imposed heavy pressure on the SILO's OCC protocol; OCC enforced transactions to abort even if single tuple of their read-set is invalidated by updaters. Meanwhile, ERMIA endured the contention by protecting the read-sets from updaters effectively with snapshot isolation and distributed the contentions across multiple versions. 
%ERMIA-SI produces commits more than ten times than SILO at 20\% scan range. %TODO. put SSI number also.
we now focus on the throughput of the analytic transactions to see how well both systems support analytics on transactional data set. As shown in the \figref{fig-tpce-robustness}(right), the analytic transactions were shoot down massively even under small contention degree on SILO andthe analytic transaction throughput drops more sharply than ERMIA, as we double transaction footprint. We can derive that the massive reader aborts critically affected the overall throughput. This is obviously a negative implication to run heterogenous workload under OCC. ERMIA provides balanced query/transaction performance, while SILO extremely gives penalty to read-intensive long transactions. 

% scalability 
\labeledfigurewide{fig-tpce-scalability}{Throughput when running TPCE-hybrid(left); TPCE-original(right)}
\labeledfigurewide{fig-tpcc-scalability}{Throughput when running TPCC-contention(left); TPCC-partitioned(right)}
In \figref{fig-tpce-scalability}(left), we fix contention degree to 20\% scan range and see scalability trends, increasing the number of workers. Overwhelmed by contentions, SILO does not achieve linear scalability, even with its excellent scalability of the underlying physical layer. ERMIA benefits from its robust CC scheme and its storage manager did not forestall achieving linear scalability. This figure shows that not only the scalability of underlying physical layer, but also CC scheme's capability to deal with contention in logical level dictates overall performance. We also performed the same experiment in the original TPC-E, without the analytic transaction. As illustrated in \figref{fig-tpce-scalability}(right), both ERMIA-SI and SILO achieve linear scalability over 24 cores. SILO does not suffer from contention, since TPCE-original has insignificant contentions. ERMIA-SSI delivered 83\% of ERMIA-SI performance with 24 threads due to the serializability cost.

\subsection{TPC-C} 
\seclabel{eval:tpcc}
We also run TPC-C where lightweight OCC camp shine. This experiment will be focusing on evaluating storage manager's scalability naturally, as TPC-C imposes little pressure on CC scheme. TPC-C is well-known for update-heavy workload and small transaction footprints. Also, it is well-partitionable workload; conflicts can be reduced even further by partitioning and single-threadinging on the partition, setting the almost ideal environment for the lightweight OCC. The only source of contention under partitioned TPC-C is multi-partition transaction, however, its impact on CC is dampened by small transaction footprints, avoiding the majority of read-write conflicts. 

% TPCC-original
We measure throughput of both systems in such partitioned TPC-C(TPCC-partitioned), varying the number of worker threads. All worker threads are given home warehouse and does not change the home warehouse during runtime. Cross partition transaction takes ??\%. \figref{fig-tpcc-scalability}(right) shows that both systems scale reasonably over 24 cores. Compared to SILO, ERMIA falls behind SILO by 10\% and 20\%, with SI and SSI respectively, at the peak performance for the following reasons. First, CC did not suppress throughput due to lack of contention. Second, SILO benefit from TPC-C's small cache footprint. Unlike SILO, ERMIA maintains multi-versions in the indirection array. It pays additional cache miss costs during traversing indirection array to find a visible version. This was invisible in the TPC-E because its large cache footprint supressed cache-optimization effect. Thus, SILO outperforms ERMIA in workloads where 1) contention is rare and 2) transaction footprint is small enough to be cached; TPC-C is the compelling example of such workloads. ERMIA-SSI had extra 10\% cost to guarantee serializability due to serial dependency checking.

% TPCC-random WH
We now change workers-warehouse binding; we enforced worker threads to pick a working partition before starting transactions randomly, following non-uniform distribution. The purpose of this modification is to bring reasonable amount of contentions in TPC-C and to see how the previous results change. We can see in the \figref{fig-tpcc-scalability}(right), the performance gap gets smaller than TPCC-partitioned as SILO is affected by the contention again. In TPCC-contention, the throughput of SILO decreased by approximately 30\%, while ERMIA delivered 15\% lower performance than in TPCC-partitioned. 

\subsection{Performance study}
\seclabel{eval:perf-study}
We performed factor analysis to figure out how much cost ERMIA pays for its critical components quantitatively. \figref{fig-tpcc-cycle} illustrates the CPU cycle breakdown(height is normalized) in the TPCC-partitioned with 24 worker threads. As shown in the figure, the indirection array pays additional 10\% costs which mainly came from last-level cache misses. This is consistent with last level cache miss breakdown in \figref{fig-tpcc-llc}. ERMIA-SSI's serializability brought additional 10\% overhead. 

The overhead of log manager was negligible and able to sustain more than 400Ktps. This shows that we do not pay too much for total ordering, which is a foundation of SI. Epoch-based TID allocator had also marginal cost(?\%). 
